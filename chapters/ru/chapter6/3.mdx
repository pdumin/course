<FrameworkSwitchCourse {fw} />

# –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –±—ã—Å—Ç—Ä—ã—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ [[fast-tokenizers-special-powers]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section3_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section3_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section3_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section3_tf.ipynb"},
]} />

{/if}

–í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ –º—ã –±–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω–æ —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ –≤ ü§ó Transformers. –î–æ —Å–∏—Ö –ø–æ—Ä –º—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –∏—Ö —Ç–æ–ª—å–∫–æ –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –æ–±—Ä–∞—Ç–Ω–æ –≤ —Ç–µ–∫—Å—Ç, –Ω–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã, –æ—Å–æ–±–µ–Ω–Ω–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–æ–π ü§ó Tokenizers, –º–æ–≥—É—Ç –¥–µ–ª–∞—Ç—å –≥–æ—Ä–∞–∑–¥–æ –±–æ–ª—å—à–µ. –ß—Ç–æ–±—ã –ø—Ä–æ–∏–ª–ª—é—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å —ç—Ç–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏, –º—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∞–π–ø–ª–∞–π–Ω–æ–≤ `token-classification` (–∫–æ—Ç–æ—Ä—É—é –º—ã –Ω–∞–∑–≤–∞–ª–∏ `NER`) –∏ `question-answering`, —Å –∫–æ—Ç–æ—Ä—ã–º–∏ –º—ã –≤–ø–µ—Ä–≤—ã–µ —Å—Ç–æ–ª–∫–Ω—É–ª–∏—Å—å –≤ [–ì–ª–∞–≤–µ 1](/course/ru/chapter1).

<Youtube id="g8quOxoqhHQ"/>

–í –¥–∞–ª—å–Ω–µ–π—à–µ–º –æ–±—Å—É–∂–¥–µ–Ω–∏–∏ –º—ã —á–∞—Å—Ç–æ –±—É–¥–µ–º –ø—Ä–æ–≤–æ–¥–∏—Ç—å —Ä–∞–∑–ª–∏—á–∏–µ –º–µ–∂–¥—É ¬´–º–µ–¥–ª–µ–Ω–Ω—ã–º–∏¬ª –∏ ¬´–±—ã—Å—Ç—Ä—ã–º–∏¬ª —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞–º–∏. –ú–µ–¥–ª–µ–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã ‚Äî —ç—Ç–æ —Ç–µ, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞–ø–∏—Å–∞–Ω—ã –Ω–∞ Python –≤–Ω—É—Ç—Ä–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ ü§ó Transformers, –∞ –±—ã—Å—Ç—Ä—ã–µ –≤–µ—Ä—Å–∏–∏ ‚Äî —ç—Ç–æ —Ç–µ, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç—Å—è ü§ó Tokenizers, –Ω–∞–ø–∏—Å–∞–Ω–Ω—ã–º–∏ –Ω–∞ Rust. –ï—Å–ª–∏ –≤—ã –ø–æ–º–Ω–∏—Ç–µ —Ç–∞–±–ª–∏—Ü—É –∏–∑ [–ì–ª–∞–≤—ã 5](/course/ru/chapter5/3), –≤ –∫–æ—Ç–æ—Ä–æ–π —Å–æ–æ–±—â–∞–ª–æ—Å—å, —Å–∫–æ–ª—å–∫–æ –≤—Ä–µ–º–µ–Ω–∏ –ø–æ—Ç—Ä–µ–±–æ–≤–∞–ª–æ—Å—å –±—ã—Å—Ç—Ä–æ–º—É –∏ –º–µ–¥–ª–µ–Ω–Ω–æ–º—É —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞–º –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –æ–±–∑–æ—Ä–∞ –ª–µ–∫–∞—Ä—Å—Ç–≤, –≤—ã –¥–æ–ª–∂–Ω—ã –∏–º–µ—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ —Ç–æ–º, –ø–æ—á–µ–º—É –º—ã –Ω–∞–∑—ã–≤–∞–µ–º –∏—Ö –±—ã—Å—Ç—Ä—ã–º–∏ –∏ –º–µ–¥–ª–µ–Ω–Ω—ã–º–∏:

                | –ë—ã—Å—Ç—Ä—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä | –ú–µ–¥–ª–µ–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
:--------------:|:--------------:|:-------------:
`batched=True`  | 10.8s          | 4min41s
`batched=False` | 59.2s          | 5min3s

<Tip warning={true}>

‚ö†Ô∏è –ü—Ä–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –æ–¥–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤—ã –Ω–µ –≤—Å–µ–≥–¥–∞ —É–≤–∏–¥–∏—Ç–µ —Ä–∞–∑–Ω–∏—Ü—É –≤ —Å–∫–æ—Ä–æ—Å—Ç–∏ –º–µ–∂–¥—É –º–µ–¥–ª–µ–Ω–Ω–æ–π –∏ –±—ã—Å—Ç—Ä–æ–π –≤–µ—Ä—Å–∏—è–º–∏ –æ–¥–Ω–æ–≥–æ –∏ —Ç–æ–≥–æ –∂–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞. –ù–∞ —Å–∞–º–æ–º –¥–µ–ª–µ, –±—ã—Å—Ç—Ä–∞—è –≤–µ—Ä—Å–∏—è –º–æ–∂–µ—Ç –±—ã—Ç—å –º–µ–¥–ª–µ–Ω–Ω–µ–µ! –¢–æ–ª—å–∫–æ –ø—Ä–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –±–æ–ª—å—à–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–µ–∫—Å—Ç–æ–≤ –≤—ã —Å–º–æ–∂–µ—Ç–µ —á–µ—Ç–∫–æ —É–≤–∏–¥–µ—Ç—å —Ä–∞–∑–Ω–∏—Ü—É.

</Tip>

## Batch encoding[[batch-encoding]] (–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –±–∞—Ç—á–µ–π)

<Youtube id="3umI3tm27Vw"/>

–í—ã–≤–æ–¥ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ ‚Äî —ç—Ç–æ –Ω–µ –ø—Ä–æ—Å—Ç–æ–π —Å–ª–æ–≤–∞—Ä—å Python; —Ç–æ, —á—Ç–æ –º—ã –ø–æ–ª—É—á–∞–µ–º, –Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ —è–≤–ª—è–µ—Ç—Å—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º –æ–±—ä–µ–∫—Ç–æ–º `BatchEncoding`. –≠—Ç–æ –ø–æ–¥–∫–ª–∞—Å—Å —Å–ª–æ–≤–∞—Ä—è (–≤–æ—Ç –ø–æ—á–µ–º—É —Ä–∞–Ω—å—à–µ –º—ã –º–æ–≥–ª–∏ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å —ç—Ç–æ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –±–µ–∑ –∫–∞–∫–∏—Ö-–ª–∏–±–æ –ø—Ä–æ–±–ª–µ–º), –Ω–æ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–µ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –±—ã—Å—Ç—Ä—ã–º–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞–º–∏.

–ü–æ–º–∏–º–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏–∏, –∫–ª—é—á–µ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –±—ã—Å—Ç—Ä—ã—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ –æ–Ω–∏ –≤—Å–µ–≥–¥–∞ –æ—Ç—Å–ª–µ–∂–∏–≤–∞—é—Ç –∏—Å—Ö–æ–¥–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω —Ç–µ–∫—Å—Ç–æ–≤, –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö –±–µ—Ä—É—Ç—Å—è –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã ‚Äî —Ñ—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä—É—é –º—ã –Ω–∞–∑—ã–≤–∞–µ–º *offset mapping* (*–æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º —Å–º–µ—â–µ–Ω–∏—è*). –≠—Ç–æ, –≤ —Å–≤–æ—é –æ—á–µ—Ä–µ–¥—å, –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –¥–æ—Å—Ç—É–ø –∫ —Ç–∞–∫–∏–º —Ñ—É–Ω–∫—Ü–∏—è–º, –∫–∞–∫ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ —Å–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∏–º —Ç–æ–∫–µ–Ω–∞–º–∏ –∏–ª–∏ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å —Ç–æ–∫–µ–Ω–æ–º, –≤ –∫–æ—Ç–æ—Ä–æ–º –æ–Ω –Ω–∞—Ö–æ–¥–∏—Ç—Å—è, –∏ –Ω–∞–æ–±–æ—Ä–æ—Ç.

–ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –ø—Ä–∏–º–µ—Ä:

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
encoding = tokenizer(example)
print(type(encoding))
```

–ö–∞–∫ —É–ø–æ–º–∏–Ω–∞–ª–æ—Å—å —Ä–∞–Ω–µ–µ, –º—ã –ø–æ–ª—É—á–∞–µ–º –æ–±—ä–µ–∫—Ç `BatchEncoding` –Ω–∞ –≤—ã—Ö–æ–¥–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞:

```python out
<class 'transformers.tokenization_utils_base.BatchEncoding'>
```

–ü–æ—Å–∫–æ–ª—å–∫—É –∫–ª–∞—Å—Å `AutoTokenizer` –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤—ã–±–∏—Ä–∞–µ—Ç –±—ã—Å—Ç—Ä—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ–º—ã–µ –æ–±—ä–µ–∫—Ç–æ–º `BatchEncoding`. –£ –Ω–∞—Å –µ—Å—Ç—å –¥–≤–∞ —Å–ø–æ—Å–æ–±–∞ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –Ω–∞—à —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –±—ã—Å—Ç—Ä—ã–º –∏–ª–∏ –º–µ–¥–ª–µ–Ω–Ω—ã–º. –ú—ã –º–æ–∂–µ–º –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∞—Ç—Ä–∏–±—É—Ç `is_fast` —É `tokenizer`:

```python
tokenizer.is_fast
```

```python out
True
```

–∏–ª–∏ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ç–æ—Ç –∂–µ –∞—Ç—Ä–∏–±—É—Ç —É –æ–±—ä–µ–∫—Ç–∞ `encoding`:

```python
encoding.is_fast
```

```python out
True
```

–î–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º, —á—Ç–æ –Ω–∞–º –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–µ–ª–∞—Ç—å –±—ã—Å—Ç—Ä—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä. –í–æ-–ø–µ—Ä–≤—ã—Ö, –º—ã –º–æ–∂–µ–º –ø–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø –∫ —Ç–æ–∫–µ–Ω–∞–º –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –æ–±—Ä–∞—Ç–Ω–æ –≤ —Ç–æ–∫–µ–Ω—ã:

```py
encoding.tokens()
```

```python out
['[CLS]', 'My', 'name', 'is', 'S', '##yl', '##va', '##in', 'and', 'I', 'work', 'at', 'Hu', '##gging', 'Face', 'in',
 'Brooklyn', '.', '[SEP]']
```

–í —ç—Ç–æ–º —Å–ª—É—á–∞–µ —Ç–æ–∫–µ–Ω –≤ –∏–Ω–¥–µ–∫—Å–µ 5 ‚Äî —ç—Ç–æ `##yl`, –∫–æ—Ç–æ—Ä—ã–π —è–≤–ª—è–µ—Ç—Å—è —á–∞—Å—Ç—å—é —Å–ª–æ–≤–∞ ¬´Sylvain¬ª –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏. –ú—ã —Ç–∞–∫–∂–µ –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ—Ç–æ–¥ `word_ids()`, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –∏–Ω–¥–µ–∫—Å —Å–ª–æ–≤–∞, –∏–∑ –∫–æ—Ç–æ—Ä–æ–≥–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω:

```py
encoding.word_ids()
```

```python out
[None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None]
```

–ú—ã –≤–∏–¥–∏–º, —á—Ç–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ `[CLS]` –∏ `[SEP]` —Å–æ–ø–æ—Å—Ç–∞–≤–ª—è—é—Ç—Å—è —Å `None`, –∞ –∑–∞—Ç–µ–º –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω —Å–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è —Å–æ —Å–ª–æ–≤–æ–º, –∏–∑ –∫–æ—Ç–æ—Ä–æ–≥–æ –æ–Ω –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç. –≠—Ç–æ –æ—Å–æ–±–µ–Ω–Ω–æ –ø–æ–ª–µ–∑–Ω–æ, —á—Ç–æ–±—ã –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ª–∏ —Ç–æ–∫–µ–Ω –≤ –Ω–∞—á–∞–ª–µ —Å–ª–æ–≤–∞ –∏–ª–∏ –¥–≤–∞ —Ç–æ–∫–µ–Ω–∞ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ –æ–¥–Ω–æ–º —Å–ª–æ–≤–µ. –ú—ã –º–æ–≥–ª–∏ –±—ã –ø–æ–ª–∞–≥–∞—Ç—å—Å—è –Ω–∞ –ø—Ä–µ—Ñ–∏–∫—Å `##` –¥–ª—è —ç—Ç–æ–≥–æ, –Ω–æ –æ–Ω —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ –¥–ª—è BERT-–ø–æ–¥–æ–±–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤; —ç—Ç–æ—Ç –º–µ—Ç–æ–¥ —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è –ª—é–±–æ–≥–æ —Ç–∏–ø–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞, –µ—Å–ª–∏ –æ–Ω –±—ã—Å—Ç—Ä—ã–π. –í —Å–ª–µ–¥—É—é—â–µ–π –≥–ª–∞–≤–µ –º—ã —É–≤–∏–¥–∏–º, –∫–∞–∫ –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç—É –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –º–µ—Ç–æ–∫, –∫–æ—Ç–æ—Ä—ã–µ —É –Ω–∞—Å –µ—Å—Ç—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞, –∫ —Ç–æ–∫–µ–Ω–∞–º –≤ —Ç–∞–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö, –∫–∞–∫ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π (NER) –∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∞—Å—Ç–µ–π —Ä–µ—á–∏ (POS). –ú—ã —Ç–∞–∫–∂–µ –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–≥–æ –¥–ª—è –º–∞—Å–∫–∏—Ä–æ–≤–∫–∏ –≤—Å–µ—Ö —Ç–æ–∫–µ–Ω–æ–≤, –∏—Å—Ö–æ–¥—è—â–∏—Ö –æ—Ç –æ–¥–Ω–æ–≥–æ –∏ —Ç–æ–≥–æ –∂–µ —Å–ª–æ–≤–∞, –ø—Ä–∏ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (–º–µ—Ç–æ–¥, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π _whole word masking_ (_–º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ–º –≤—Å–µ–≥–æ —Å–ª–æ–≤–∞_)).

<Tip>

–ü–æ–Ω—è—Ç–∏–µ –æ —Ç–æ–º, —á—Ç–æ —Ç–∞–∫–æ–µ —Å–ª–æ–≤–æ, —Å–ª–æ–∂–Ω–æ–µ. –ù–∞–ø—Ä–∏–º–µ—Ä, —Å—á–∏—Ç–∞–µ—Ç—Å—è –ª–∏ ¬´I'll¬ª (—Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ –æ—Ç ¬´I will¬ª) –æ–¥–Ω–∏–º –∏–ª–∏ –¥–≤—É–º—è —Å–ª–æ–≤–∞–º–∏? –ù–∞ —Å–∞–º–æ–º –¥–µ–ª–µ —ç—Ç–æ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –ø—Ä–∏–º–µ–Ω—è–µ–º–æ–π –∏–º –æ–ø–µ—Ä–∞—Ü–∏–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏. –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã –ø—Ä–æ—Å—Ç–æ —Ä–∞–∑–±–∏–≤–∞—é—Ç –Ω–∞ –ø—Ä–æ–±–µ–ª—ã, –ø–æ—ç—Ç–æ–º—É –æ–Ω–∏ –±—É–¥—É—Ç —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å —ç—Ç–æ –∫–∞–∫ –æ–¥–Ω–æ —Å–ª–æ–≤–æ. –î—Ä—É–≥–∏–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –ø–æ–≤–µ—Ä—Ö –ø—Ä–æ–±–µ–ª–æ–≤, –ø–æ—ç—Ç–æ–º—É –±—É–¥–µ–º —Å—á–∏—Ç–∞—Ç—å —ç—Ç–æ –¥–≤—É–º—è —Å–ª–æ–≤–∞–º–∏.

‚úèÔ∏è **–ü–æ–ø—Ä–æ–±—É–π—Ç–µ!** –°–æ–∑–¥–∞–π—Ç–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–∑ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã—Ö —Ç–æ—á–µ–∫ `bert-base-cased` –∏ `roberta-base` –∏ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–π—Ç–µ —Å –∏—Ö –ø–æ–º–æ—â—å—é ¬´81s¬ª. –ß—Ç–æ –≤—ã –Ω–∞–±–ª—é–¥–∞–µ—Ç–µ? 

</Tip>

–¢–æ—á–Ω–æ —Ç–∞–∫ –∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –º–µ—Ç–æ–¥ `sentence_ids()`, –∫–æ—Ç–æ—Ä—ã–π –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∞ —Å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º, –∏–∑ –∫–æ—Ç–æ—Ä–æ–≥–æ –æ–Ω –±—ã–ª –ø–æ–ª—É—á–µ–Ω (—Ö–æ—Ç—è –≤ —ç—Ç–æ–º —Å–ª—É—á–∞–µ `token_type_ids`, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º, –º–æ–∂–µ—Ç –¥–∞—Ç—å –Ω–∞–º —Ç—É –∂–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é).

–ù–∞–∫–æ–Ω–µ—Ü, –º—ã –º–æ–∂–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –ª—é–±–æ–µ —Å–ª–æ–≤–æ –∏–ª–∏ —Ç–æ–∫–µ–Ω –≤ —Å–∏–º–≤–æ–ª—ã –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Ç–µ–∫—Å—Ç–µ –∏ –Ω–∞–æ–±–æ—Ä–æ—Ç —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–æ–≤ ]`word_to_chars()` –∏–ª–∏ `token_to_chars()` –∏ `char_to_word()` –∏–ª–∏ `char_to_token()`. –ù–∞–ø—Ä–∏–º–µ—Ä, –º–µ—Ç–æ–¥ `word_ids()` —Å–æ–æ–±—â–∏–ª –Ω–∞–º, —á—Ç–æ `##yl` —è–≤–ª—è–µ—Ç—Å—è —á–∞—Å—Ç—å—é —Å–ª–æ–≤–∞ —Å –∏–Ω–¥–µ–∫—Å–æ–º 3, –Ω–æ –∫–∞–∫–æ–µ —ç—Ç–æ —Å–ª–æ–≤–æ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏? –ú—ã –º–æ–∂–µ–º —É–∑–Ω–∞—Ç—å —Ç–∞–∫:

```py
start, end = encoding.word_to_chars(3)
example[start:end]
```

```python out
Sylvain
```

–ö–∞–∫ –º—ã —É–ø–æ–º–∏–Ω–∞–ª–∏ —Ä–∞–Ω–µ–µ, –≤—Å–µ —ç—Ç–æ –æ—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ —Ç–æ–º —Ñ–∞–∫—Ç–µ, —á—Ç–æ –±—ã—Å—Ç—Ä—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç –¥–∏–∞–ø–∞–∑–æ–Ω —Ç–µ–∫—Å—Ç–∞, –∏–∑ –∫–æ—Ç–æ—Ä–æ–≥–æ –ø–æ—Å—Ç—É–ø–∞–µ—Ç –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω, –≤ —Å–ø–∏—Å–∫–µ *offsets* (*—Å–º–µ—â–µ–Ω–∏–π*). –ß—Ç–æ–±—ã –ø—Ä–æ–∏–ª–ª—é—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å –∏—Ö –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ, –¥–∞–ª–µ–µ –º—ã –ø–æ–∫–∞–∂–µ–º, –∫–∞–∫ –≤—Ä—É—á–Ω—É—é –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–æ–Ω–≤–µ–π–µ—Ä–∞ `token-classification`.

<Tip>

‚úèÔ∏è **–ü–æ–ø—Ä–æ–±—É–π—Ç–µ!** –°–æ–∑–¥–∞–π—Ç–µ —Å–≤–æ–π —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π –ø—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –∏ –ø–æ—Å–º–æ—Ç—Ä–∏—Ç–µ, —Å–º–æ–∂–µ—Ç–µ –ª–∏ –≤—ã –ø–æ–Ω—è—Ç—å, –∫–∞–∫–∏–µ —Ç–æ–∫–µ–Ω—ã —Å–≤—è–∑–∞–Ω—ã —Å –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–º —Å–ª–æ–≤–∞, –∞ —Ç–∞–∫–∂–µ –∫–∞–∫ –∏–∑–≤–ª–µ—á—å –¥–∏–∞–ø–∞–∑–æ–Ω—ã —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Å–ª–æ–≤–∞. –í –∫–∞—á–µ—Å—Ç–≤–µ –±–æ–Ω—É—Å–∞ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–≤–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∫–∞–∫ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –ø–æ—Å–º–æ—Ç—Ä–∏—Ç–µ, –ø–æ–Ω—è—Ç–Ω—ã –ª–∏ –≤–∞–º –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.

</Tip>

## `token-classification` –∫–∞–∫ —á–∞—Å—Ç—å –ø–∞–π–ø–ª–∞–π–Ω–∞ [[inside-the-token-classification-pipeline]]

–í [–ì–ª–∞–≤–µ 1](/course/ru/chapter1) –º—ã –≤–ø–µ—Ä–≤—ã–µ –ø–æ–ø—Ä–æ–±–æ–≤–∞–ª–∏ –ø—Ä–∏–º–µ–Ω–∏—Ç—å NER ‚Äî –≥–¥–µ –∑–∞–¥–∞—á–∞ —Å–æ—Å—Ç–æ–∏—Ç –≤ —Ç–æ–º, —á—Ç–æ–±—ã –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, –∫–∞–∫–∏–µ —á–∞—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —Ç–∞–∫–∏–º —Å—É—â–Ω–æ—Å—Ç—è–º, –∫–∞–∫ –ª—é–¥–∏, –º–µ—Å—Ç–∞ –∏–ª–∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ ‚Äî —Å –ø–∞–π–ø–ª–∞–π–Ω–æ–º ü§ó Transformers `pipeline()`. –ó–∞—Ç–µ–º, –≤ [–ì–ª–∞–≤–µ 2](/course/ru/chapter2), –º—ã —É–≤–∏–¥–µ–ª–∏, –∫–∞–∫ –∫–æ–Ω–≤–µ–π–µ—Ä –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ç—Ä–∏ —ç—Ç–∞–ø–∞, –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –∏–∑ –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è, –ø–µ—Ä–µ–¥–∞—á–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å –∏ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞. –ü–µ—Ä–≤—ã–µ –¥–≤–∞ —à–∞–≥–∞ –≤ –∫–æ–Ω–≤–µ–π–µ—Ä–µ `token-classification` —Ç–∞–∫–∏–µ –∂–µ, –∫–∞–∫ –∏ –≤ –ª—é–±–æ–º –¥—Ä—É–≥–æ–º –∫–æ–Ω–≤–µ–π–µ—Ä–µ, –Ω–æ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ–º–Ω–æ–≥–æ —Å–ª–æ–∂–Ω–µ–µ ‚Äî –¥–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º!

{#if fw === 'pt'}

<Youtube id="0E7ltQB7fM8"/>

{:else}

<Youtube id="PrX4CjrVnNc"/>

{/if}

### –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Å –ø–æ–º–æ—â—å—é –ø–∞–π–ø–ª–∞–π–Ω–∞[[getting-the-base-results-with-the-pipeline]]

–í–æ-–ø–µ—Ä–≤—ã—Ö, –¥–∞–≤–∞–π—Ç–µ –≤–æ–∑—å–º–µ–º –ø–∞–π–ø–ª–∞–π–Ω –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤, —á—Ç–æ–±—ã –º—ã –º–æ–≥–ª–∏ –ø–æ–ª—É—á–∏—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–æ–¥–µ–ª—å [`dbmdz/bert-large-cased-finetuned-conll03-english`](https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english); –æ–Ω–∞ —Ä–µ—à–∞–µ—Ç –∑–∞–¥–∞—á—É NER –¥–ª—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π:

```py
from transformers import pipeline

token_classifier = pipeline("token-classification")
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python out
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

–ú–æ–¥–µ–ª—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–ª–∞ –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π ¬´Sylvain¬ª, –∫–∞–∫ —á–µ–ª–æ–≤–µ–∫–∞, –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π ¬´Hugging Face¬ª, –∫–∞–∫ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é, –∞ —Ç–æ–∫–µ–Ω ¬´Brooklyn¬ª –∫–∞–∫ –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ. –ú—ã —Ç–∞–∫–∂–µ –º–æ–∂–µ–º –ø–æ–ø—Ä–æ—Å–∏—Ç—å –∫–æ–Ω–≤–µ–π–µ—Ä —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–∫–µ–Ω—ã, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –æ–¥–Ω–æ–º—É –∏ —Ç–æ–º—É –∂–µ –æ–±—ä–µ–∫—Ç—É:

```py
from transformers import pipeline

token_classifier = pipeline("token-classification", aggregation_strategy="simple")
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python out
[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

–í—ã–±—Ä–∞–Ω–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è `aggregation_strategy` –∏–∑–º–µ–Ω–∏—Ç –æ—Ü–µ–Ω–∫–∏, –≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞. –í —Å–ª—É—á–∞–µ `simple` –æ—Ü–µ–Ω–∫–∞ ‚Äî —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫ –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –≤ –¥–∞–Ω–Ω–æ–º –æ–±—ä–µ–∫—Ç–µ: –Ω–∞–ø—Ä–∏–º–µ—Ä, –æ—Ü–µ–Ω–∫–∞ ¬´Sylvain¬ª ‚Äî —ç—Ç–æ —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫, –∫–æ—Ç–æ—Ä—ã–µ –º—ã –≤–∏–¥–µ–ª–∏ –≤ –ø—Ä–µ–¥—ã–¥—É—â–µ–º –ø—Ä–∏–º–µ—Ä–µ –¥–ª—è —Ç–æ–∫–µ–Ω–æ–≤ ¬´S¬ª. , `##yl`, `##va` –∏ `##in`. –î—Ä—É–≥–∏–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏:

- `"first"`: –æ—Ü–µ–Ω–∫–∞ –∫–∞–∂–¥–æ–π —Å—É—â–Ω–æ—Å—Ç–∏ ‚Äî —ç—Ç–æ –æ—Ü–µ–Ω–∫–∞ –ø–µ—Ä–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ —ç—Ç–æ–π —Å—É—â–Ω–æ—Å—Ç–∏ (—Ç–∞–∫, –¥–ª—è ¬´Sylvain¬ª —ç—Ç–æ –±—É–¥–µ—Ç 0.993828, –æ—Ü–µ–Ω–∫–∞ —Ç–æ–∫–µ–Ω–∞ ¬´S¬ª)
- `"max"`: –æ—Ü–µ–Ω–∫–∞ –∫–∞–∂–¥–æ–π —Å—É—â–Ω–æ—Å—Ç–∏ ‚Äî —ç—Ç–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —ç—Ç–æ–π —Å—É—â–Ω–æ—Å—Ç–∏ (—Ç–∞–∫, –¥–ª—è ¬´Hugging Face¬ª —ç—Ç–æ –±—É–¥–µ—Ç 0.98879766, –æ—Ü–µ–Ω–∫–∞ ¬´Face¬ª)
- `"average"`: –≥–¥–µ –æ—Ü–µ–Ω–∫–∞ –∫–∞–∂–¥–æ–π —Å—É—â–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Å—Ä–µ–¥–Ω—é—é –æ—Ü–µ–Ω–∫—É —Å–ª–æ–≤, —Å–æ—Å—Ç–∞–≤–ª—è—é—â–∏—Ö —ç—Ç—É —Å—É—â–Ω–æ—Å—Ç—å (—Ç–∞–∫, –¥–ª—è ¬´Sylvain¬ª –Ω–µ –±—É–¥–µ—Ç –Ω–∏–∫–∞–∫–æ–π —Ä–∞–∑–Ω–∏—Ü—ã —Å `simple` —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π, –Ω–æ ¬´Hugging Face¬ª –±—É–¥–µ—Ç –∏–º–µ—Ç—å –æ—Ü–µ–Ω–∫—É 0.9819, —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –±–∞–ª–ª–æ–≤ –∑–∞ ¬´Hugging¬ª, 0.975, –∏ ¬´Face¬ª, 0.98879)

–¢–µ–ø–µ—Ä—å –¥–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫ –ø–æ–ª—É—á–∏—Ç—å —ç—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ `pipeline()`!

### –û—Ç –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º[[from-inputs-to-predictions]]

{#if fw === 'pt'}

–°–Ω–∞—á–∞–ª–∞ –Ω–∞–º –Ω—É–∂–Ω–æ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å –Ω–∞—à –≤–≤–æ–¥ –∏ –ø–µ—Ä–µ–¥–∞—Ç—å –µ–≥–æ —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å. –≠—Ç–æ –¥–µ–ª–∞–µ—Ç—Å—è —Ç–æ—á–Ω–æ —Ç–∞–∫ –∂–µ, –∫–∞–∫ –≤ [–ì–ª–∞–≤–µ 2](/course/ru/chapter2); –º—ã —Å–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏, –∏—Å–ø–æ–ª—å–∑—É—è –∫–ª–∞—Å—Å—ã `AutoXxx`, –∞ –∑–∞—Ç–µ–º –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö –≤ –Ω–∞—à–µ–º –ø—Ä–∏–º–µ—Ä–µ:

```py
from transformers import AutoTokenizer, AutoModelForTokenClassification

model_checkpoint = "dbmdz/bert-large-cased-finetuned-conll03-english"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)

example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
inputs = tokenizer(example, return_tensors="pt")
outputs = model(**inputs)
```

–ü–æ—Å–∫–æ–ª—å–∫—É –∑–¥–µ—Å—å –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º `AutoModelForTokenClassification`, –º—ã –ø–æ–ª—É—á–∞–µ–º –æ–¥–∏–Ω –Ω–∞–±–æ—Ä –ª–æ–≥–∏—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –≤–æ –≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:

```py
print(inputs["input_ids"].shape)
print(outputs.logits.shape)
```

```python out
torch.Size([1, 19])
torch.Size([1, 19, 9])
```

{:else}

–°–Ω–∞—á–∞–ª–∞ –Ω–∞–º –Ω—É–∂–Ω–æ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å –Ω–∞—à –≤–≤–æ–¥ –∏ –ø–µ—Ä–µ–¥–∞—Ç—å –µ–≥–æ —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å. –≠—Ç–æ –¥–µ–ª–∞–µ—Ç—Å—è —Ç–æ—á–Ω–æ —Ç–∞–∫ –∂–µ, –∫–∞–∫ –≤ [–ì–ª–∞–≤–µ 2](/course/ru/chapter2); –º—ã —Å–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏, –∏—Å–ø–æ–ª—å–∑—É—è –∫–ª–∞—Å—Å—ã `TFAutoXxx`, –∞ –∑–∞—Ç–µ–º –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö –≤ –Ω–∞—à–µ–º –ø—Ä–∏–º–µ—Ä–µ:

```py
from transformers import AutoTokenizer, TFAutoModelForTokenClassification

model_checkpoint = "dbmdz/bert-large-cased-finetuned-conll03-english"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = TFAutoModelForTokenClassification.from_pretrained(model_checkpoint)

example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
inputs = tokenizer(example, return_tensors="tf")
outputs = model(**inputs)
```

–ü–æ—Å–∫–æ–ª—å–∫—É –∑–¥–µ—Å—å –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º `TFAutoModelForTokenClassification` here, –º—ã –ø–æ–ª—É—á–∞–µ–º –æ–¥–∏–Ω –Ω–∞–±–æ—Ä –ª–æ–≥–∏—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –≤–æ –≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:

```py
print(inputs["input_ids"].shape)
print(outputs.logits.shape)
```

```python out
(1, 19)
(1, 19, 9)
```

{/if}

–£ –Ω–∞—Å –µ—Å—Ç—å –±–∞—Ç—á —Å 1 –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –∏–∑ 19 —Ç–æ–∫–µ–Ω–æ–≤, –∞ –º–æ–¥–µ–ª—å –∏–º–µ–µ—Ç 9 —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Ç–æ–∫, –ø–æ—ç—Ç–æ–º—É –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏–º–µ—é—Ç —Ñ–æ—Ä–º—É 1 x 19 x 9. –ö–∞–∫ –∏ –¥–ª—è –∫–æ–Ω–≤–µ–π–µ—Ä–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞, –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é softmax –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —ç—Ç–∏—Ö –ª–æ–≥–∏—Ç–æ–≤ –∫ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º, –∏ –º—ã –±–µ—Ä–µ–º argmax –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ (–æ–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –º—ã –º–æ–∂–µ–º –≤–∑—è—Ç—å argmax –¥–ª—è –ª–æ–≥–∏—Ç–æ–≤, –ø–æ—Ç–æ–º—É —á—Ç–æ softmax –Ω–µ –º–µ–Ω—è–µ—Ç –ø–æ—Ä—è–¥–æ–∫):

{#if fw === 'pt'}

```py
import torch

probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()
predictions = outputs.logits.argmax(dim=-1)[0].tolist()
print(predictions)
```

{:else}

```py
import tensorflow as tf

probabilities = tf.math.softmax(outputs.logits, axis=-1)[0]
probabilities = probabilities.numpy().tolist()
predictions = tf.math.argmax(outputs.logits, axis=-1)[0]
predictions = predictions.numpy().tolist()
print(predictions)
```

{/if}

```python out
[0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0]
```

–ê—Ç—Ä–∏–±—É—Ç `model.config.id2label` —Å–æ–¥–µ—Ä–∂–∏–∏—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –∏–Ω–¥–µ–∫—Å–æ–≤ –∏ –ª–µ–π–±–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:

```py
model.config.id2label
```

```python out
{0: 'O',
 1: 'B-MISC',
 2: 'I-MISC',
 3: 'B-PER',
 4: 'I-PER',
 5: 'B-ORG',
 6: 'I-ORG',
 7: 'B-LOC',
 8: 'I-LOC'}
```

–ö–∞–∫ –º—ã –≤–∏–¥–µ–ª–∏ —Ä–∞–Ω–µ–µ, –∏–º–µ–µ—Ç—Å—è 9 –º–µ—Ç–æ–∫: ¬´O¬ª ‚Äî —ç—Ç–æ –º–µ—Ç–∫–∞ –¥–ª—è —Ç–æ–∫–µ–Ω–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –Ω–∏ –≤ –æ–¥–Ω–æ–π –∏–º–µ–Ω–æ–≤–∞–Ω–Ω–æ–π —Å—É—â–Ω–æ—Å—Ç–∏ (—ç—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç ¬´outside¬ª), –∏ —É –Ω–∞—Å –µ—Å—Ç—å –¥–≤–µ –º–µ—Ç–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞ —Å—É—â–Ω–æ—Å—Ç–∏ (—Ä–∞–∑–Ω–æ–µ, —á–µ–ª–æ–≤–µ–∫, –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è –∏ –º–µ—Å—Ç–æ–Ω–∞—Ö–æ–∂–¥–µ–Ω–∏–µ). –ú–µ—Ç–∫–∞ ¬´B-XXX¬ª —É–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Ç–æ–∫–µ–Ω –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –Ω–∞—á–∞–ª–µ —Å—É—â–Ω–æ—Å—Ç–∏ ¬´XXX¬ª, –∞ –º–µ—Ç–∫–∞ ¬´I-XXX¬ª —É–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Ç–æ–∫–µ–Ω –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ —Å—É—â–Ω–æ—Å—Ç–∏ ¬´XXX¬ª. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ —Ç–µ–∫—É—â–µ–º –ø—Ä–∏–º–µ—Ä–µ –º—ã –æ–∂–∏–¥–∞–µ–º, —á—Ç–æ –Ω–∞—à–∞ –º–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Ç–æ–∫–µ–Ω `S` –∫–∞–∫ `B-PER` (–Ω–∞—á–∞–ª–æ –ª–∏—á–Ω–æ—Å—Ç–∏), –∞ —Ç–æ–∫–µ–Ω—ã `##yl`, `##va` –∏ `## in` –∫–∞–∫ `I-PER` (–≤–Ω—É—Ç—Ä–∏ –ª–∏—á–Ω–æ—Å—Ç–∏).

–í—ã –º–æ–∂–µ—Ç–µ –ø–æ–¥—É–º–∞—Ç—å, —á—Ç–æ –≤ —ç—Ç–æ–º —Å–ª—É—á–∞–µ –º–æ–¥–µ–ª—å –±—ã–ª–∞ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–π, –ø–æ—Å–∫–æ–ª—å–∫—É –æ–Ω–∞ –¥–∞–ª–∞ –º–µ—Ç–∫—É ¬´I-PER¬ª –≤—Å–µ–º —á–µ—Ç—ã—Ä–µ–º —ç—Ç–∏–º —Ç–æ–∫–µ–Ω–∞–º, –Ω–æ —ç—Ç–æ –Ω–µ —Å–æ–≤—Å–µ–º —Ç–∞–∫. –ù–∞ —Å–∞–º–æ–º –¥–µ–ª–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –¥–≤–∞ —Ñ–æ—Ä–º–∞—Ç–∞ –¥–ª—è —ç—Ç–∏—Ö –º–µ—Ç–æ–∫ `B-` –∏ `I-`: *IOB1* –∏ *IOB2*. –§–æ—Ä–º–∞—Ç IOB2 (—Ä–æ–∑–æ–≤—ã–π –Ω–∏–∂–µ) ‚Äî —ç—Ç–æ —Ç–æ—Ç, –∫–æ—Ç–æ—Ä—ã–π –º—ã –≤–≤–µ–ª–∏, —Ç–æ–≥–¥–∞ –∫–∞–∫ –≤ —Ñ–æ—Ä–º–∞—Ç–µ IOB1 (—Å–∏–Ω–∏–π) –º–µ—Ç–∫–∏, –Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è —Å `B-`, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –¥–≤—É—Ö —Å–º–µ–∂–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –æ–¥–Ω–æ–≥–æ —Ç–∏–ø–∞. –ú–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä—É—é –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º, –±—ã–ª–∞ —Ç–æ—á–Ω–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞ –Ω–∞ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —ç—Ç–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞, –ø–æ—ç—Ç–æ–º—É –æ–Ω–∞ –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ—Ç –º–µ—Ç–∫—É `I-PER` –º–∞—Ä–∫–µ—Ä—É `S`.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions.svg" alt="IOB1 vs IOB2 format"/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions-dark.svg" alt="IOB1 vs IOB2 format"/>
</div>

–° –ø–æ–º–æ—â—å—é —ç—Ç–æ–≥–æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –º—ã –≥–æ—Ç–æ–≤—ã –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ—Å—Ç–∏ (–ø–æ—á—Ç–∏ –ø–æ–ª–Ω–æ—Å—Ç—å—é) —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–µ—Ä–≤–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ ‚Äî –º—ã –º–æ–∂–µ–º –ø—Ä–æ—Å—Ç–æ –ø–æ–ª—É—á–∏—Ç—å –æ—Ü–µ–Ω–∫—É –∏ –º–µ—Ç–∫—É –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞, –∫–æ—Ç–æ—Ä—ã–π –Ω–µ –±—ã–ª –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω –∫–∞–∫ `O`:

```py
results = []
tokens = inputs.tokens()

for idx, pred in enumerate(predictions):
    label = model.config.id2label[pred]
    if label != "O":
        results.append(
            {"entity": label, "score": probabilities[idx][pred], "word": tokens[idx]}
        )

print(results)
```

```python out
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S'},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl'},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va'},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in'},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu'},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging'},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face'},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn'}]
```

–≠—Ç–æ –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–µ –Ω–∞ —Ç–æ, —á—Ç–æ —É –Ω–∞—Å –±—ã–ª–æ —Ä–∞–Ω—å—à–µ, –∑–∞ –æ–¥–Ω–∏–º –∏—Å–∫–ª—é—á–µ–Ω–∏–µ–º: –ø–∞–π–ø–ª–∞–π–Ω —Ç–∞–∫–∂–µ –¥–∞–ª –Ω–∞–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ `–Ω–∞—á–∞–ª–µ` –∏ `–∫–æ–Ω—Ü–µ` –∫–∞–∂–¥–æ–π —Å—É—â–Ω–æ—Å—Ç–∏ –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏. –ó–¥–µ—Å—å –≤ –∏–≥—Ä—É –≤—Å—Ç—É–ø–∏—Ç –Ω–∞—à–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–º–µ—â–µ–Ω–∏—è. –ß—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å —Å–º–µ—â–µ–Ω–∏—è, –Ω–∞–º –ø—Ä–æ—Å—Ç–æ –Ω—É–∂–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å `return_offsets_mapping=True`, –∫–æ–≥–¥–∞ –º—ã –ø—Ä–∏–º–µ–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∫ –Ω–∞—à–∏–º –≤—Ö–æ–¥–∞–º:

```py
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
inputs_with_offsets["offset_mapping"]
```

```python out
[(0, 0), (0, 2), (3, 7), (8, 10), (11, 12), (12, 14), (14, 16), (16, 18), (19, 22), (23, 24), (25, 29), (30, 32),
 (33, 35), (35, 40), (41, 45), (46, 48), (49, 57), (57, 58), (0, 0)]
```

–ö–∞–∂–¥—ã–π –∫–æ—Ä—Ç–µ–∂ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç —Ç–µ–∫—Å—Ç–∞, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –∫–∞–∂–¥–æ–º—É —Ç–æ–∫–µ–Ω—É, –≥–¥–µ `(0, 0)` –∑–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤. –†–∞–Ω–µ–µ –º—ã –≤–∏–¥–µ–ª–∏, —á—Ç–æ —Ç–æ–∫–µ–Ω —Å –∏–Ω–¥–µ–∫—Å–æ–º 5 ‚Äî —ç—Ç–æ `##yl`, –∫–æ—Ç–æ—Ä—ã–π –∑–¥–µ—Å—å –∏–º–µ–µ—Ç `(12, 14)` –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Å–º–µ—â–µ–Ω–∏—è. –ï—Å–ª–∏ –º—ã –≤–æ–∑—å–º–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç –≤ –Ω–∞—à–µ–º –ø—Ä–∏–º–µ—Ä–µ:

```py
example[12:14]
```
–º—ã –ø–æ–ª—É—á–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω —Ç–µ–∫—Å—Ç–∞ –±–µ–∑ `##`:

```python out
yl
```

–ò—Å–ø–æ–ª—å–∑—É—è —ç—Ç–æ, –º—ã —Ç–µ–ø–µ—Ä—å –º–æ–∂–µ–º –∑–∞–≤–µ—Ä—à–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç—å –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:

```py
results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets["offset_mapping"]

for idx, pred in enumerate(predictions):
    label = model.config.id2label[pred]
    if label != "O":
        start, end = offsets[idx]
        results.append(
            {
                "entity": label,
                "score": probabilities[idx][pred],
                "word": tokens[idx],
                "start": start,
                "end": end,
            }
        )

print(results)
```

```python out
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

–¢–æ –∂–µ —Å–∞–º–æ–µ, —á—Ç–æ –º—ã –ø–æ–ª—É—á–∏–ª–∏ –æ—Ç –ø–µ—Ä–≤–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞!

### –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π[[grouping-entities]]

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å–º–µ—â–µ–Ω–∏–π –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ –∏ –∫–æ–Ω–µ—á–Ω–æ–≥–æ –∫–ª—é—á–µ–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞ —É–¥–æ–±–Ω–æ, –Ω–æ —ç—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Å—Ç—Ä–æ–≥–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–π. –û–¥–Ω–∞–∫–æ –∫–æ–≥–¥–∞ –º—ã —Ö–æ—Ç–∏–º —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å –æ–±—ä–µ–∫—Ç—ã –≤–º–µ—Å—Ç–µ, —Å–º–µ—â–µ–Ω–∏—è –∏–∑–±–∞–≤—è—Ç –Ω–∞—Å –æ—Ç –±–æ–ª—å—à–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –±–µ—Å–ø–æ—Ä—è–¥–æ—á–Ω–æ–≥–æ –∫–æ–¥–∞. –ù–∞–ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ –º—ã —Ö–æ—Ç–∏–º —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–∫–µ–Ω—ã `Hu`, `##gging` –∏ `Face`, –º—ã –º–æ–∂–µ–º —Å–æ–∑–¥–∞—Ç—å —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞, –∫–æ—Ç–æ—Ä—ã–µ –≥–æ–≤–æ—Ä—è—Ç, —á—Ç–æ –ø–µ—Ä–≤—ã–µ –¥–≤–∞ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø—Ä–∏—Å–æ–µ–¥–∏–Ω–µ–Ω—ã –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ `##`, –∞ `Face` –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –¥–æ–±–∞–≤–ª–µ–Ω–æ —á–µ—Ä–µ–∑ –ø—Ä–æ–±–µ–ª, —Ç–∞–∫ –∫–∞–∫ –æ–Ω–æ –Ω–µ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å `##`, –Ω–æ —ç—Ç–æ –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Ç–æ–ª—å–∫–æ –¥–ª—è —ç—Ç–æ–≥–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ç–∏–ø–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞. –ù–∞–º –ø—Ä–∏—à–ª–æ—Å—å –±—ã –Ω–∞–ø–∏—Å–∞—Ç—å –µ—â–µ –æ–¥–∏–Ω –Ω–∞–±–æ—Ä –ø—Ä–∞–≤–∏–ª –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ SentencePiece –∏–ª–∏ Byte-Pair-Encoding (–æ–±—Å—É–∂–¥–∞–µ—Ç—Å—è –¥–∞–ª–µ–µ –≤ —ç—Ç–æ–π –≥–ª–∞–≤–µ).

–°–æ —Å–º–µ—â–µ–Ω–∏—è–º–∏ –≤–µ—Å—å —ç—Ç–æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∫–æ–¥ –∏—Å—á–µ–∑–∞–µ—Ç: –º—ã –ø—Ä–æ—Å—Ç–æ –º–æ–∂–µ–º –≤–∑—è—Ç—å –¥–∏–∞–ø–∞–∑–æ–Ω –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Ç–µ–∫—Å—Ç–µ, –∫–æ—Ç–æ—Ä—ã–π –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –ø–µ—Ä–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –∏ –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –ø–æ—Å–ª–µ–¥–Ω–∏–º —Ç–æ–∫–µ–Ω–æ–º. –ò—Ç–∞–∫, –≤ —Å–ª—É—á–∞–µ —Ç–æ–∫–µ–Ω–æ–≤ `Hu`, `##gging` –∏ `Face` –º—ã –¥–æ–ª–∂–Ω—ã –Ω–∞—á–∏–Ω–∞—Ç—å —Å 33-–≥–æ —Å–∏–º–≤–æ–ª–∞ (–Ω–∞—á–∞–ª–æ `Hu`) –∏ –∑–∞–∫–∞–Ω—á–∏–≤–∞—Ç—å –ø–µ—Ä–µ–¥ 45-–º —Å–∏–º–≤–æ–ª–æ–º (–∫–æ–Ω–µ—Ü `Face`):

```py
example[33:45]
```

```python out
Hugging Face
```

–ß—Ç–æ–±—ã –Ω–∞–ø–∏—Å–∞—Ç—å –∫–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –≤—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫—É –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –ø—Ä–∏ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–µ –æ–±—ä–µ–∫—Ç–æ–≤, –º—ã —Å–≥—Ä—É–ø–ø–∏—Ä—É–µ–º –æ–±—ä–µ–∫—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ —Å–ª–µ–¥—É—é—Ç –¥—Ä—É–≥ –∑–∞ –¥—Ä—É–≥–æ–º –∏ –ø–æ–º–µ—á–µ–Ω—ã `I-XXX`, –∑–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ–º –ø–µ—Ä–≤–æ–≥–æ, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ–º–µ—á–µ–Ω –∫–∞–∫ `B-XXX` –∏–ª–∏ `I-XXX` (—Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –º—ã –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å —Å—É—â–Ω–æ—Å—Ç—å, –∫–æ–≥–¥–∞ –ø–æ–ª—É—á–∞–µ–º `O`, –Ω–æ–≤—ã–π —Ç–∏–ø —Å—É—â–Ω–æ—Å—Ç–∏ –∏–ª–∏ `B-XXX`, –∫–æ—Ç–æ—Ä—ã–µ –≥–æ–≤–æ—Ä—è—Ç –Ω–∞–º, —á—Ç–æ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å—É—â–Ω–æ—Å—Ç—å —Ç–æ–≥–æ –∂–µ —Ç–∏–ø–∞):

```py
import numpy as np

results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets["offset_mapping"]

idx = 0
while idx < len(predictions):
    pred = predictions[idx]
    label = model.config.id2label[pred]
    if label != "O":
        # –£–¥–∞–ª—è–µ–º  B- –∏–ª–∏ I-
        label = label[2:]
        start, _ = offsets[idx]

        # –ó–∞–±–∏—Ä–∞–µ–º –≤—Å–µ —Ç–æ–∫–µ–Ω—ã —Å I-
        all_scores = []
        while (
            idx < len(predictions)
            and model.config.id2label[predictions[idx]] == f"I-{label}"
        ):
            all_scores.append(probabilities[idx][pred])
            _, end = offsets[idx]
            idx += 1

        # Score - —Å—Ä–µ–¥–Ω–∏–π score –¥–ª—è –≤—Å–µ—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—É—â–Ω–æ—Å—Ç–∏
        score = np.mean(all_scores).item()
        word = example[start:end]
        results.append(
            {
                "entity_group": label,
                "score": score,
                "word": word,
                "start": start,
                "end": end,
            }
        )
    idx += 1

print(results)
```

–ò –º—ã –ø–æ–ª—É—á–∏–ª–∏ —Ç–∞–∫–æ–π –∂–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –∫–∞–∫ –∏ –≤–æ –≤—Ç–æ—Ä–æ–º –ø–∞–π–ø–ª–∞–π–Ω–µ!

```python out
[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

–î—Ä—É–≥–æ–π –ø—Ä–∏–º–µ—Ä –∑–∞–¥–∞—á–∏, –≥–¥–µ —ç—Ç–∏ —Å–º–µ—â–µ–Ω–∏—è —á—Ä–µ–∑–≤—ã—á–∞–π–Ω–æ –ø–æ–ª–µ–∑–Ω—ã, ‚Äî —ç—Ç–æ –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã. –ü–æ–≥—Ä—É–∂–µ–Ω–∏–µ –≤ —ç—Ç–æ—Ç –∫–æ–Ω–≤–µ–π–µ—Ä, –∫–æ—Ç–æ—Ä–æ–µ –º—ã —Å–¥–µ–ª–∞–µ–º –≤ —Å–ª–µ–¥—É—é—â–µ–º —Ä–∞–∑–¥–µ–ª–µ, —Ç–∞–∫–∂–µ –ø–æ–∑–≤–æ–ª–∏—Ç –Ω–∞–º –≤–∑–≥–ª—è–Ω—É—Ç—å –Ω–∞ –æ–¥–Ω—É –ø–æ—Å–ª–µ–¥–Ω—é—é —Ñ—É–Ω–∫—Ü–∏—é —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫–µ ü§ó Transformers: –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤ –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏—è, –∫–æ–≥–¥–∞ –º—ã —É—Å–µ–∫–∞–µ–º –≤–≤–æ–¥ –¥–æ –∑–∞–¥–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã.
