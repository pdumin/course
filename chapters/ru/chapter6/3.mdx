<FrameworkSwitchCourse {fw} />

# –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –±—ã—Å—Ç—Ä—ã—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ [[fast-tokenizers-special-powers]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section3_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section3_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section3_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section3_tf.ipynb"},
]} />

{/if}

–í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ –º—ã –±–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω–æ —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ –≤ ü§ó Transformers. –î–æ —Å–∏—Ö –ø–æ—Ä –º—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –∏—Ö —Ç–æ–ª—å–∫–æ –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –æ–±—Ä–∞—Ç–Ω–æ –≤ —Ç–µ–∫—Å—Ç, –Ω–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã, –æ—Å–æ–±–µ–Ω–Ω–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–æ–π ü§ó Tokenizers, –º–æ–≥—É—Ç –¥–µ–ª–∞—Ç—å –≥–æ—Ä–∞–∑–¥–æ –±–æ–ª—å—à–µ. –ß—Ç–æ–±—ã –ø—Ä–æ–∏–ª–ª—é—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å —ç—Ç–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏, –º—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∞–π–ø–ª–∞–π–Ω–æ–≤ `token-classification` (–∫–æ—Ç–æ—Ä—É—é –º—ã –Ω–∞–∑–≤–∞–ª–∏ `NER`) –∏ `question-answering`, —Å –∫–æ—Ç–æ—Ä—ã–º–∏ –º—ã –≤–ø–µ—Ä–≤—ã–µ —Å—Ç–æ–ª–∫–Ω—É–ª–∏—Å—å –≤ [–ì–ª–∞–≤–µ 1](/course/ru/chapter1).

<Youtube id="g8quOxoqhHQ"/>

–í –¥–∞–ª—å–Ω–µ–π—à–µ–º –æ–±—Å—É–∂–¥–µ–Ω–∏–∏ –º—ã —á–∞—Å—Ç–æ –±—É–¥–µ–º –ø—Ä–æ–≤–æ–¥–∏—Ç—å —Ä–∞–∑–ª–∏—á–∏–µ –º–µ–∂–¥—É ¬´–º–µ–¥–ª–µ–Ω–Ω—ã–º–∏¬ª –∏ ¬´–±—ã—Å—Ç—Ä—ã–º–∏¬ª —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞–º–∏. –ú–µ–¥–ª–µ–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã ‚Äî —ç—Ç–æ —Ç–µ, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞–ø–∏—Å–∞–Ω—ã –Ω–∞ Python –≤–Ω—É—Ç—Ä–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ ü§ó Transformers, –∞ –±—ã—Å—Ç—Ä—ã–µ –≤–µ—Ä—Å–∏–∏ ‚Äî —ç—Ç–æ —Ç–µ, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç—Å—è ü§ó Tokenizers, –Ω–∞–ø–∏—Å–∞–Ω–Ω—ã–º–∏ –Ω–∞ Rust. –ï—Å–ª–∏ –≤—ã –ø–æ–º–Ω–∏—Ç–µ —Ç–∞–±–ª–∏—Ü—É –∏–∑ [–ì–ª–∞–≤—ã 5](/course/ru/chapter5/3), –≤ –∫–æ—Ç–æ—Ä–æ–π —Å–æ–æ–±—â–∞–ª–æ—Å—å, —Å–∫–æ–ª—å–∫–æ –≤—Ä–µ–º–µ–Ω–∏ –ø–æ—Ç—Ä–µ–±–æ–≤–∞–ª–æ—Å—å –±—ã—Å—Ç—Ä–æ–º—É –∏ –º–µ–¥–ª–µ–Ω–Ω–æ–º—É —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞–º –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –æ–±–∑–æ—Ä–∞ –ª–µ–∫–∞—Ä—Å—Ç–≤, –≤—ã –¥–æ–ª–∂–Ω—ã –∏–º–µ—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ —Ç–æ–º, –ø–æ—á–µ–º—É –º—ã –Ω–∞–∑—ã–≤–∞–µ–º –∏—Ö –±—ã—Å—Ç—Ä—ã–º–∏ –∏ –º–µ–¥–ª–µ–Ω–Ω—ã–º–∏:

                | –ë—ã—Å—Ç—Ä—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä | –ú–µ–¥–ª–µ–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
:--------------:|:--------------:|:-------------:
`batched=True`  | 10.8s          | 4min41s
`batched=False` | 59.2s          | 5min3s

<Tip warning={true}>

‚ö†Ô∏è –ü—Ä–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –æ–¥–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤—ã –Ω–µ –≤—Å–µ–≥–¥–∞ —É–≤–∏–¥–∏—Ç–µ —Ä–∞–∑–Ω–∏—Ü—É –≤ —Å–∫–æ—Ä–æ—Å—Ç–∏ –º–µ–∂–¥—É –º–µ–¥–ª–µ–Ω–Ω–æ–π –∏ –±—ã—Å—Ç—Ä–æ–π –≤–µ—Ä—Å–∏—è–º–∏ –æ–¥–Ω–æ–≥–æ –∏ —Ç–æ–≥–æ –∂–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞. –ù–∞ —Å–∞–º–æ–º –¥–µ–ª–µ, –±—ã—Å—Ç—Ä–∞—è –≤–µ—Ä—Å–∏—è –º–æ–∂–µ—Ç –±—ã—Ç—å –º–µ–¥–ª–µ–Ω–Ω–µ–µ! –¢–æ–ª—å–∫–æ –ø—Ä–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –±–æ–ª—å—à–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–µ–∫—Å—Ç–æ–≤ –≤—ã —Å–º–æ–∂–µ—Ç–µ —á–µ—Ç–∫–æ —É–≤–∏–¥–µ—Ç—å —Ä–∞–∑–Ω–∏—Ü—É.

</Tip>

## Batch encoding[[batch-encoding]] (–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –±–∞—Ç—á–µ–π)

<Youtube id="3umI3tm27Vw"/>

–í—ã–≤–æ–¥ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ ‚Äî —ç—Ç–æ –Ω–µ –ø—Ä–æ—Å—Ç–æ–π —Å–ª–æ–≤–∞—Ä—å Python; —Ç–æ, —á—Ç–æ –º—ã –ø–æ–ª—É—á–∞–µ–º, –Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ —è–≤–ª—è–µ—Ç—Å—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º –æ–±—ä–µ–∫—Ç–æ–º `BatchEncoding`. –≠—Ç–æ –ø–æ–¥–∫–ª–∞—Å—Å —Å–ª–æ–≤–∞—Ä—è (–≤–æ—Ç –ø–æ—á–µ–º—É —Ä–∞–Ω—å—à–µ –º—ã –º–æ–≥–ª–∏ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å —ç—Ç–æ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –±–µ–∑ –∫–∞–∫–∏—Ö-–ª–∏–±–æ –ø—Ä–æ–±–ª–µ–º), –Ω–æ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–µ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –±—ã—Å—Ç—Ä—ã–º–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞–º–∏.

–ü–æ–º–∏–º–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏–∏, –∫–ª—é—á–µ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –±—ã—Å—Ç—Ä—ã—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ –æ–Ω–∏ –≤—Å–µ–≥–¥–∞ –æ—Ç—Å–ª–µ–∂–∏–≤–∞—é—Ç –∏—Å—Ö–æ–¥–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω —Ç–µ–∫—Å—Ç–æ–≤, –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö –±–µ—Ä—É—Ç—Å—è –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã ‚Äî —Ñ—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä—É—é –º—ã –Ω–∞–∑—ã–≤–∞–µ–º *offset mapping* (*–æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º —Å–º–µ—â–µ–Ω–∏—è*). –≠—Ç–æ, –≤ —Å–≤–æ—é –æ—á–µ—Ä–µ–¥—å, –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –¥–æ—Å—Ç—É–ø –∫ —Ç–∞–∫–∏–º —Ñ—É–Ω–∫—Ü–∏—è–º, –∫–∞–∫ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ —Å–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∏–º —Ç–æ–∫–µ–Ω–∞–º–∏ –∏–ª–∏ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å —Ç–æ–∫–µ–Ω–æ–º, –≤ –∫–æ—Ç–æ—Ä–æ–º –æ–Ω –Ω–∞—Ö–æ–¥–∏—Ç—Å—è, –∏ –Ω–∞–æ–±–æ—Ä–æ—Ç.

–ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –ø—Ä–∏–º–µ—Ä:

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
encoding = tokenizer(example)
print(type(encoding))
```

–ö–∞–∫ —É–ø–æ–º–∏–Ω–∞–ª–æ—Å—å —Ä–∞–Ω–µ–µ, –º—ã –ø–æ–ª—É—á–∞–µ–º –æ–±—ä–µ–∫—Ç `BatchEncoding` –Ω–∞ –≤—ã—Ö–æ–¥–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞:

```python out
<class 'transformers.tokenization_utils_base.BatchEncoding'>
```

–ü–æ—Å–∫–æ–ª—å–∫—É –∫–ª–∞—Å—Å `AutoTokenizer` –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤—ã–±–∏—Ä–∞–µ—Ç –±—ã—Å—Ç—Ä—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ–º—ã–µ –æ–±—ä–µ–∫—Ç–æ–º `BatchEncoding`. –£ –Ω–∞—Å –µ—Å—Ç—å –¥–≤–∞ —Å–ø–æ—Å–æ–±–∞ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –Ω–∞—à —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –±—ã—Å—Ç—Ä—ã–º –∏–ª–∏ –º–µ–¥–ª–µ–Ω–Ω—ã–º. –ú—ã –º–æ–∂–µ–º –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∞—Ç—Ä–∏–±—É—Ç `is_fast` —É `tokenizer`:

```python
tokenizer.is_fast
```

```python out
True
```

–∏–ª–∏ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ç–æ—Ç –∂–µ –∞—Ç—Ä–∏–±—É—Ç —É –æ–±—ä–µ–∫—Ç–∞ `encoding`:

```python
encoding.is_fast
```

```python out
True
```

–î–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º, —á—Ç–æ –Ω–∞–º –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–µ–ª–∞—Ç—å –±—ã—Å—Ç—Ä—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä. –í–æ-–ø–µ—Ä–≤—ã—Ö, –º—ã –º–æ–∂–µ–º –ø–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø –∫ —Ç–æ–∫–µ–Ω–∞–º –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –æ–±—Ä–∞—Ç–Ω–æ –≤ —Ç–æ–∫–µ–Ω—ã:

```py
encoding.tokens()
```

```python out
['[CLS]', 'My', 'name', 'is', 'S', '##yl', '##va', '##in', 'and', 'I', 'work', 'at', 'Hu', '##gging', 'Face', 'in',
 'Brooklyn', '.', '[SEP]']
```

–í —ç—Ç–æ–º —Å–ª—É—á–∞–µ —Ç–æ–∫–µ–Ω –≤ –∏–Ω–¥–µ–∫—Å–µ 5 ‚Äî —ç—Ç–æ `##yl`, –∫–æ—Ç–æ—Ä—ã–π —è–≤–ª—è–µ—Ç—Å—è —á–∞—Å—Ç—å—é —Å–ª–æ–≤–∞ ¬´Sylvain¬ª –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏. –ú—ã —Ç–∞–∫–∂–µ –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ—Ç–æ–¥ `word_ids()`, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –∏–Ω–¥–µ–∫—Å —Å–ª–æ–≤–∞, –∏–∑ –∫–æ—Ç–æ—Ä–æ–≥–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω:

```py
encoding.word_ids()
```

```python out
[None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None]
```

–ú—ã –≤–∏–¥–∏–º, —á—Ç–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ `[CLS]` –∏ `[SEP]` —Å–æ–ø–æ—Å—Ç–∞–≤–ª—è—é—Ç—Å—è —Å `None`, –∞ –∑–∞—Ç–µ–º –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω —Å–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è —Å–æ —Å–ª–æ–≤–æ–º, –∏–∑ –∫–æ—Ç–æ—Ä–æ–≥–æ –æ–Ω –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç. –≠—Ç–æ –æ—Å–æ–±–µ–Ω–Ω–æ –ø–æ–ª–µ–∑–Ω–æ, —á—Ç–æ–±—ã –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ª–∏ —Ç–æ–∫–µ–Ω –≤ –Ω–∞—á–∞–ª–µ —Å–ª–æ–≤–∞ –∏–ª–∏ –¥–≤–∞ —Ç–æ–∫–µ–Ω–∞ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ –æ–¥–Ω–æ–º —Å–ª–æ–≤–µ. –ú—ã –º–æ–≥–ª–∏ –±—ã –ø–æ–ª–∞–≥–∞—Ç—å—Å—è –Ω–∞ –ø—Ä–µ—Ñ–∏–∫—Å `##` –¥–ª—è —ç—Ç–æ–≥–æ, –Ω–æ –æ–Ω —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ –¥–ª—è BERT-–ø–æ–¥–æ–±–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤; —ç—Ç–æ—Ç –º–µ—Ç–æ–¥ —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è –ª—é–±–æ–≥–æ —Ç–∏–ø–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞, –µ—Å–ª–∏ –æ–Ω –±—ã—Å—Ç—Ä—ã–π. –í —Å–ª–µ–¥—É—é—â–µ–π –≥–ª–∞–≤–µ –º—ã —É–≤–∏–¥–∏–º, –∫–∞–∫ –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç—É –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –º–µ—Ç–æ–∫, –∫–æ—Ç–æ—Ä—ã–µ —É –Ω–∞—Å –µ—Å—Ç—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞, –∫ —Ç–æ–∫–µ–Ω–∞–º –≤ —Ç–∞–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö, –∫–∞–∫ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π (NER) –∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∞—Å—Ç–µ–π —Ä–µ—á–∏ (POS). –ú—ã —Ç–∞–∫–∂–µ –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–≥–æ –¥–ª—è –º–∞—Å–∫–∏—Ä–æ–≤–∫–∏ –≤—Å–µ—Ö —Ç–æ–∫–µ–Ω–æ–≤, –∏—Å—Ö–æ–¥—è—â–∏—Ö –æ—Ç –æ–¥–Ω–æ–≥–æ –∏ —Ç–æ–≥–æ –∂–µ —Å–ª–æ–≤–∞, –ø—Ä–∏ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (–º–µ—Ç–æ–¥, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π _whole word masking_ (_–º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ–º –≤—Å–µ–≥–æ —Å–ª–æ–≤–∞_)).

<Tip>

–ü–æ–Ω—è—Ç–∏–µ –æ —Ç–æ–º, —á—Ç–æ —Ç–∞–∫–æ–µ —Å–ª–æ–≤–æ, —Å–ª–æ–∂–Ω–æ–µ. –ù–∞–ø—Ä–∏–º–µ—Ä, —Å—á–∏—Ç–∞–µ—Ç—Å—è –ª–∏ ¬´I'll¬ª (—Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ –æ—Ç ¬´I will¬ª) –æ–¥–Ω–∏–º –∏–ª–∏ –¥–≤—É–º—è —Å–ª–æ–≤–∞–º–∏? –ù–∞ —Å–∞–º–æ–º –¥–µ–ª–µ —ç—Ç–æ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –ø—Ä–∏–º–µ–Ω—è–µ–º–æ–π –∏–º –æ–ø–µ—Ä–∞—Ü–∏–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏. –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã –ø—Ä–æ—Å—Ç–æ —Ä–∞–∑–±–∏–≤–∞—é—Ç –Ω–∞ –ø—Ä–æ–±–µ–ª—ã, –ø–æ—ç—Ç–æ–º—É –æ–Ω–∏ –±—É–¥—É—Ç —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å —ç—Ç–æ –∫–∞–∫ –æ–¥–Ω–æ —Å–ª–æ–≤–æ. –î—Ä—É–≥–∏–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –ø–æ–≤–µ—Ä—Ö –ø—Ä–æ–±–µ–ª–æ–≤, –ø–æ—ç—Ç–æ–º—É –±—É–¥–µ–º —Å—á–∏—Ç–∞—Ç—å —ç—Ç–æ –¥–≤—É–º—è —Å–ª–æ–≤–∞–º–∏.

‚úèÔ∏è **–ü–æ–ø—Ä–æ–±—É–π—Ç–µ!** –°–æ–∑–¥–∞–π—Ç–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–∑ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã—Ö —Ç–æ—á–µ–∫ `bert-base-cased` –∏ `roberta-base` –∏ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–π—Ç–µ —Å –∏—Ö –ø–æ–º–æ—â—å—é ¬´81s¬ª. –ß—Ç–æ –≤—ã –Ω–∞–±–ª—é–¥–∞–µ—Ç–µ? 

</Tip>

–¢–æ—á–Ω–æ —Ç–∞–∫ –∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –º–µ—Ç–æ–¥ `sentence_ids()`, –∫–æ—Ç–æ—Ä—ã–π –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∞ —Å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º, –∏–∑ –∫–æ—Ç–æ—Ä–æ–≥–æ –æ–Ω –±—ã–ª –ø–æ–ª—É—á–µ–Ω (—Ö–æ—Ç—è –≤ —ç—Ç–æ–º —Å–ª—É—á–∞–µ `token_type_ids`, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º, –º–æ–∂–µ—Ç –¥–∞—Ç—å –Ω–∞–º —Ç—É –∂–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é).

–ù–∞–∫–æ–Ω–µ—Ü, –º—ã –º–æ–∂–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –ª—é–±–æ–µ —Å–ª–æ–≤–æ –∏–ª–∏ —Ç–æ–∫–µ–Ω –≤ —Å–∏–º–≤–æ–ª—ã –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Ç–µ–∫—Å—Ç–µ –∏ –Ω–∞–æ–±–æ—Ä–æ—Ç —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–æ–≤ ]`word_to_chars()` –∏–ª–∏ `token_to_chars()` –∏ `char_to_word()` –∏–ª–∏ `char_to_token()`. –ù–∞–ø—Ä–∏–º–µ—Ä, –º–µ—Ç–æ–¥ `word_ids()` —Å–æ–æ–±—â–∏–ª –Ω–∞–º, —á—Ç–æ `##yl` —è–≤–ª—è–µ—Ç—Å—è —á–∞—Å—Ç—å—é —Å–ª–æ–≤–∞ —Å –∏–Ω–¥–µ–∫—Å–æ–º 3, –Ω–æ –∫–∞–∫–æ–µ —ç—Ç–æ —Å–ª–æ–≤–æ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏? –ú—ã –º–æ–∂–µ–º —É–∑–Ω–∞—Ç—å —Ç–∞–∫:

```py
start, end = encoding.word_to_chars(3)
example[start:end]
```

```python out
Sylvain
```

–ö–∞–∫ –º—ã —É–ø–æ–º–∏–Ω–∞–ª–∏ —Ä–∞–Ω–µ–µ, –≤—Å–µ —ç—Ç–æ –æ—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ —Ç–æ–º —Ñ–∞–∫—Ç–µ, —á—Ç–æ –±—ã—Å—Ç—Ä—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç –¥–∏–∞–ø–∞–∑–æ–Ω —Ç–µ–∫—Å—Ç–∞, –∏–∑ –∫–æ—Ç–æ—Ä–æ–≥–æ –ø–æ—Å—Ç—É–ø–∞–µ—Ç –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω, –≤ —Å–ø–∏—Å–∫–µ *offsets* (*—Å–º–µ—â–µ–Ω–∏–π*). –ß—Ç–æ–±—ã –ø—Ä–æ–∏–ª–ª—é—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å –∏—Ö –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ, –¥–∞–ª–µ–µ –º—ã –ø–æ–∫–∞–∂–µ–º, –∫–∞–∫ –≤—Ä—É—á–Ω—É—é –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–æ–Ω–≤–µ–π–µ—Ä–∞ `token-classification`.

<Tip>

‚úèÔ∏è **–ü–æ–ø—Ä–æ–±—É–π—Ç–µ!** –°–æ–∑–¥–∞–π—Ç–µ —Å–≤–æ–π —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π –ø—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –∏ –ø–æ—Å–º–æ—Ç—Ä–∏—Ç–µ, —Å–º–æ–∂–µ—Ç–µ –ª–∏ –≤—ã –ø–æ–Ω—è—Ç—å, –∫–∞–∫–∏–µ —Ç–æ–∫–µ–Ω—ã —Å–≤—è–∑–∞–Ω—ã —Å –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–º —Å–ª–æ–≤–∞, –∞ —Ç–∞–∫–∂–µ –∫–∞–∫ –∏–∑–≤–ª–µ—á—å –¥–∏–∞–ø–∞–∑–æ–Ω—ã —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Å–ª–æ–≤–∞. –í –∫–∞—á–µ—Å—Ç–≤–µ –±–æ–Ω—É—Å–∞ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–≤–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∫–∞–∫ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –ø–æ—Å–º–æ—Ç—Ä–∏—Ç–µ, –ø–æ–Ω—è—Ç–Ω—ã –ª–∏ –≤–∞–º –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.

</Tip>

## `token-classification` –∫–∞–∫ —á–∞—Å—Ç—å –ø–∞–π–ø–ª–∞–π–Ω–∞ [[inside-the-token-classification-pipeline]]

–í [–ì–ª–∞–≤–µ 1](/course/ru/chapter1) –º—ã –≤–ø–µ—Ä–≤—ã–µ –ø–æ–ø—Ä–æ–±–æ–≤–∞–ª–∏ –ø—Ä–∏–º–µ–Ω–∏—Ç—å NER ‚Äî –≥–¥–µ –∑–∞–¥–∞—á–∞ —Å–æ—Å—Ç–æ–∏—Ç –≤ —Ç–æ–º, —á—Ç–æ–±—ã –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, –∫–∞–∫–∏–µ —á–∞—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —Ç–∞–∫–∏–º —Å—É—â–Ω–æ—Å—Ç—è–º, –∫–∞–∫ –ª—é–¥–∏, –º–µ—Å—Ç–∞ –∏–ª–∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ ‚Äî —Å –ø–∞–π–ø–ª–∞–π–Ω–æ–º ü§ó Transformers `pipeline()`. –ó–∞—Ç–µ–º, –≤ [–ì–ª–∞–≤–µ 2](/course/ru/chapter2), –º—ã —É–≤–∏–¥–µ–ª–∏, –∫–∞–∫ –∫–æ–Ω–≤–µ–π–µ—Ä –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ç—Ä–∏ —ç—Ç–∞–ø–∞, –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –∏–∑ –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è, –ø–µ—Ä–µ–¥–∞—á–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å –∏ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞. –ü–µ—Ä–≤—ã–µ –¥–≤–∞ —à–∞–≥–∞ –≤ –∫–æ–Ω–≤–µ–π–µ—Ä–µ `token-classification` —Ç–∞–∫–∏–µ –∂–µ, –∫–∞–∫ –∏ –≤ –ª—é–±–æ–º –¥—Ä—É–≥–æ–º –∫–æ–Ω–≤–µ–π–µ—Ä–µ, –Ω–æ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ–º–Ω–æ–≥–æ —Å–ª–æ–∂–Ω–µ–µ ‚Äî –¥–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º!

{#if fw === 'pt'}

<Youtube id="0E7ltQB7fM8"/>

{:else}

<Youtube id="PrX4CjrVnNc"/>

{/if}

### –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Å –ø–æ–º–æ—â—å—é –ø–∞–π–ø–ª–∞–π–Ω–∞[[getting-the-base-results-with-the-pipeline]]

–í–æ-–ø–µ—Ä–≤—ã—Ö, –¥–∞–≤–∞–π—Ç–µ –≤–æ–∑—å–º–µ–º –ø–∞–π–ø–ª–∞–π–Ω –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤, —á—Ç–æ–±—ã –º—ã –º–æ–≥–ª–∏ –ø–æ–ª—É—á–∏—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–æ–¥–µ–ª—å [`dbmdz/bert-large-cased-finetuned-conll03-english`](https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english); –æ–Ω–∞ —Ä–µ—à–∞–µ—Ç –∑–∞–¥–∞—á—É NER –¥–ª—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π:

```py
from transformers import pipeline

token_classifier = pipeline("token-classification")
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python out
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

–ú–æ–¥–µ–ª—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–ª–∞ –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π ¬´Sylvain¬ª, –∫–∞–∫ —á–µ–ª–æ–≤–µ–∫–∞, –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π ¬´Hugging Face¬ª, –∫–∞–∫ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é, –∞ —Ç–æ–∫–µ–Ω ¬´Brooklyn¬ª –∫–∞–∫ –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ. –ú—ã —Ç–∞–∫–∂–µ –º–æ–∂–µ–º –ø–æ–ø—Ä–æ—Å–∏—Ç—å –∫–æ–Ω–≤–µ–π–µ—Ä —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–∫–µ–Ω—ã, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –æ–¥–Ω–æ–º—É –∏ —Ç–æ–º—É –∂–µ –æ–±—ä–µ–∫—Ç—É:

```py
from transformers import pipeline

token_classifier = pipeline("token-classification", aggregation_strategy="simple")
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python out
[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

–í—ã–±—Ä–∞–Ω–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è `aggregation_strategy` –∏–∑–º–µ–Ω–∏—Ç –æ—Ü–µ–Ω–∫–∏, –≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞. –í —Å–ª—É—á–∞–µ `simple` –æ—Ü–µ–Ω–∫–∞ ‚Äî —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫ –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –≤ –¥–∞–Ω–Ω–æ–º –æ–±—ä–µ–∫—Ç–µ: –Ω–∞–ø—Ä–∏–º–µ—Ä, –æ—Ü–µ–Ω–∫–∞ ¬´Sylvain¬ª ‚Äî —ç—Ç–æ —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫, –∫–æ—Ç–æ—Ä—ã–µ –º—ã –≤–∏–¥–µ–ª–∏ –≤ –ø—Ä–µ–¥—ã–¥—É—â–µ–º –ø—Ä–∏–º–µ—Ä–µ –¥–ª—è —Ç–æ–∫–µ–Ω–æ–≤ ¬´S¬ª. , `##yl`, `##va` –∏ `##in`. –î—Ä—É–≥–∏–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏:

- `"first"`: –æ—Ü–µ–Ω–∫–∞ –∫–∞–∂–¥–æ–π —Å—É—â–Ω–æ—Å—Ç–∏ ‚Äî —ç—Ç–æ –æ—Ü–µ–Ω–∫–∞ –ø–µ—Ä–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ —ç—Ç–æ–π —Å—É—â–Ω–æ—Å—Ç–∏ (—Ç–∞–∫, –¥–ª—è ¬´Sylvain¬ª —ç—Ç–æ –±—É–¥–µ—Ç 0.993828, –æ—Ü–µ–Ω–∫–∞ —Ç–æ–∫–µ–Ω–∞ ¬´S¬ª)
- `"max"`: –æ—Ü–µ–Ω–∫–∞ –∫–∞–∂–¥–æ–π —Å—É—â–Ω–æ—Å—Ç–∏ ‚Äî —ç—Ç–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —ç—Ç–æ–π —Å—É—â–Ω–æ—Å—Ç–∏ (—Ç–∞–∫, –¥–ª—è ¬´Hugging Face¬ª —ç—Ç–æ –±—É–¥–µ—Ç 0.98879766, –æ—Ü–µ–Ω–∫–∞ ¬´Face¬ª)
- `"average"`: –≥–¥–µ –æ—Ü–µ–Ω–∫–∞ –∫–∞–∂–¥–æ–π —Å—É—â–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Å—Ä–µ–¥–Ω—é—é –æ—Ü–µ–Ω–∫—É —Å–ª–æ–≤, —Å–æ—Å—Ç–∞–≤–ª—è—é—â–∏—Ö —ç—Ç—É —Å—É—â–Ω–æ—Å—Ç—å (—Ç–∞–∫, –¥–ª—è ¬´Sylvain¬ª –Ω–µ –±—É–¥–µ—Ç –Ω–∏–∫–∞–∫–æ–π —Ä–∞–∑–Ω–∏—Ü—ã —Å `simple` —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π, –Ω–æ ¬´Hugging Face¬ª –±—É–¥–µ—Ç –∏–º–µ—Ç—å –æ—Ü–µ–Ω–∫—É 0.9819, —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –±–∞–ª–ª–æ–≤ –∑–∞ ¬´Hugging¬ª, 0.975, –∏ ¬´Face¬ª, 0.98879)

–¢–µ–ø–µ—Ä—å –¥–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫ –ø–æ–ª—É—á–∏—Ç—å —ç—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ `pipeline()`!

### From inputs to predictions[[from-inputs-to-predictions]]

{#if fw === 'pt'}

First we need to tokenize our input and pass it through the model. This is done exactly as in [Chapter 2](/course/chapter2); we instantiate the tokenizer and the model using the `AutoXxx` classes and then use them on our example:

```py
from transformers import AutoTokenizer, AutoModelForTokenClassification

model_checkpoint = "dbmdz/bert-large-cased-finetuned-conll03-english"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)

example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
inputs = tokenizer(example, return_tensors="pt")
outputs = model(**inputs)
```

Since we're using `AutoModelForTokenClassification` here, we get one set of logits for each token in the input sequence:

```py
print(inputs["input_ids"].shape)
print(outputs.logits.shape)
```

```python out
torch.Size([1, 19])
torch.Size([1, 19, 9])
```

{:else}

First we need to tokenize our input and pass it through the model. This is done exactly as in [Chapter 2](/course/chapter2); we instantiate the tokenizer and the model using the `TFAutoXxx` classes and then use them on our example:

```py
from transformers import AutoTokenizer, TFAutoModelForTokenClassification

model_checkpoint = "dbmdz/bert-large-cased-finetuned-conll03-english"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = TFAutoModelForTokenClassification.from_pretrained(model_checkpoint)

example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
inputs = tokenizer(example, return_tensors="tf")
outputs = model(**inputs)
```

Since we're using `TFAutoModelForTokenClassification` here, we get one set of logits for each token in the input sequence:

```py
print(inputs["input_ids"].shape)
print(outputs.logits.shape)
```

```python out
(1, 19)
(1, 19, 9)
```

{/if}

We have a batch with 1 sequence of 19 tokens and the model has 9 different labels, so the output of the model has a shape of 1 x 19 x 9. Like for the text classification pipeline, we use a softmax function to convert those logits to probabilities, and we take the argmax to get predictions (note that we can take the argmax on the logits because the softmax does not change the order):

{#if fw === 'pt'}

```py
import torch

probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()
predictions = outputs.logits.argmax(dim=-1)[0].tolist()
print(predictions)
```

{:else}

```py
import tensorflow as tf

probabilities = tf.math.softmax(outputs.logits, axis=-1)[0]
probabilities = probabilities.numpy().tolist()
predictions = tf.math.argmax(outputs.logits, axis=-1)[0]
predictions = predictions.numpy().tolist()
print(predictions)
```

{/if}

```python out
[0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0]
```

The `model.config.id2label` attribute contains the mapping of indexes to labels that we can use to make sense of the predictions:

```py
model.config.id2label
```

```python out
{0: 'O',
 1: 'B-MISC',
 2: 'I-MISC',
 3: 'B-PER',
 4: 'I-PER',
 5: 'B-ORG',
 6: 'I-ORG',
 7: 'B-LOC',
 8: 'I-LOC'}
```

As we saw earlier, there are 9 labels: `O` is the label for the tokens that are not in any named entity (it stands for "outside"), and we then have two labels for each type of entity (miscellaneous, person, organization, and location). The label `B-XXX` indicates the token is at the beginning of an entity `XXX` and the label `I-XXX` indicates the token is inside the entity `XXX`. For instance, in the current example we would expect our model to classify the token `S` as `B-PER` (beginning of a person entity) and the tokens `##yl`, `##va` and `##in` as `I-PER` (inside a person entity). 

You might think the model was wrong in this case as it gave the label `I-PER` to all four of these tokens, but that's not entirely true. There are actually two formats for those `B-` and `I-` labels: *IOB1* and *IOB2*. The IOB2 format (in pink below), is the one we introduced whereas in the IOB1 format (in blue), the labels beginning with `B-` are only ever used to separate two adjacent entities of the same type. The model we are using was fine-tuned on a dataset using that format, which is why it assigns the label `I-PER` to the `S` token.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions.svg" alt="IOB1 vs IOB2 format"/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions-dark.svg" alt="IOB1 vs IOB2 format"/>
</div>

With this map, we are ready to reproduce (almost entirely) the results of the first pipeline -- we can just grab the score and label of each token that was not classified as `O`:

```py
results = []
tokens = inputs.tokens()

for idx, pred in enumerate(predictions):
    label = model.config.id2label[pred]
    if label != "O":
        results.append(
            {"entity": label, "score": probabilities[idx][pred], "word": tokens[idx]}
        )

print(results)
```

```python out
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S'},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl'},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va'},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in'},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu'},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging'},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face'},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn'}]
```

This is very similar to what we had before, with one exception: the pipeline also gave us information about the `start` and `end` of each entity in the original sentence. This is where our offset mapping will come into play. To get the offsets, we just have to set `return_offsets_mapping=True` when we apply the tokenizer to our inputs:

```py
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
inputs_with_offsets["offset_mapping"]
```

```python out
[(0, 0), (0, 2), (3, 7), (8, 10), (11, 12), (12, 14), (14, 16), (16, 18), (19, 22), (23, 24), (25, 29), (30, 32),
 (33, 35), (35, 40), (41, 45), (46, 48), (49, 57), (57, 58), (0, 0)]
```

Each tuple is the span of text corresponding to each token, where `(0, 0)` is reserved for the special tokens. We saw before that the token at index 5 is `##yl`, which has `(12, 14)` as offsets here. If we grab the corresponding slice in our example:


```py
example[12:14]
```

we get the proper span of text without the `##`:

```python out
yl
```

Using this, we can now complete the previous results:

```py
results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets["offset_mapping"]

for idx, pred in enumerate(predictions):
    label = model.config.id2label[pred]
    if label != "O":
        start, end = offsets[idx]
        results.append(
            {
                "entity": label,
                "score": probabilities[idx][pred],
                "word": tokens[idx],
                "start": start,
                "end": end,
            }
        )

print(results)
```

```python out
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

This is the same as what we got from the first pipeline!

### Grouping entities[[grouping-entities]]

Using the offsets to determine the start and end keys for each entity is handy, but that information isn't strictly necessary. When we want to group the entities together, however, the offsets will save us a lot of messy code. For example, if we wanted to group together the tokens `Hu`, `##gging`, and `Face`, we could make special rules that say the first two should be attached while removing the `##`, and the `Face` should be added with a space since it does not begin with `##` -- but that would only work for this particular type of tokenizer. We would have to write another set of rules for a SentencePiece or a Byte-Pair-Encoding tokenizer (discussed later in this chapter).

With the offsets, all that custom code goes away: we just can take the span in the original text that begins with the first token and ends with the last token. So, in the case of the tokens `Hu`, `##gging`, and `Face`, we should start at character 33 (the beginning of `Hu`) and end before character 45 (the end of `Face`):

```py
example[33:45]
```

```python out
Hugging Face
```

To write the code that post-processes the predictions while grouping entities, we will group together entities that are consecutive and labeled with `I-XXX`, except for the first one, which can be labeled as `B-XXX` or `I-XXX` (so, we stop grouping an entity when we get a `O`, a new type of entity, or a `B-XXX` that tells us an entity of the same type is starting):

```py
import numpy as np

results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets["offset_mapping"]

idx = 0
while idx < len(predictions):
    pred = predictions[idx]
    label = model.config.id2label[pred]
    if label != "O":
        # Remove the B- or I-
        label = label[2:]
        start, _ = offsets[idx]

        # Grab all the tokens labeled with I-label
        all_scores = []
        while (
            idx < len(predictions)
            and model.config.id2label[predictions[idx]] == f"I-{label}"
        ):
            all_scores.append(probabilities[idx][pred])
            _, end = offsets[idx]
            idx += 1

        # The score is the mean of all the scores of the tokens in that grouped entity
        score = np.mean(all_scores).item()
        word = example[start:end]
        results.append(
            {
                "entity_group": label,
                "score": score,
                "word": word,
                "start": start,
                "end": end,
            }
        )
    idx += 1

print(results)
```

And we get the same results as with our second pipeline!

```python out
[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

Another example of a task where these offsets are extremely useful is question answering. Diving into that pipeline, which we'll do in the next section, will also enable us to take a look at one last feature of the tokenizers in the ü§ó Transformers library: dealing with overflowing tokens when we truncate an input to a given length.
