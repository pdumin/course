<FrameworkSwitchCourse {fw} />


# –û—Å–æ–±—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –±—ã—Å—Ç—Ä—ã—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤[[fast-tokenizers-special-powers]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section3_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section3_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section3_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section3_tf.ipynb"},
]} />

{/if}


–í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ –º—ã –ø–æ–¥—Ä–æ–±–Ω–æ —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ –≤ ü§ó Transformers. –î–æ —Å–∏—Ö –ø–æ—Ä –º—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –∏—Ö —Ç–æ–ª—å–∫–æ –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –æ–±—Ä–∞—Ç–Ω–æ –≤ —Ç–µ–∫—Å—Ç, –Ω–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã -- –æ—Å–æ–±–µ–Ω–Ω–æ —Ç–µ, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è –±–∏–±–ª–∏–æ—Ç–µ–∫–æ–π ü§ó Tokenizers - –º–æ–≥—É—Ç –¥–µ–ª–∞—Ç—å –≥–æ—Ä–∞–∑–¥–æ –±–æ–ª—å—à–µ. –ß—Ç–æ–±—ã –ø—Ä–æ–∏–ª–ª—é—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å —ç—Ç–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏, –º—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–æ–Ω–≤–µ–π–µ—Ä–æ–≤ `token-classification` (–∫–æ—Ç–æ—Ä—ã–µ –º—ã –Ω–∞–∑–≤–∞–ª–∏ `ner`) –∏ `question-answering`, —Å –∫–æ—Ç–æ—Ä—ã–º–∏ –º—ã –≤–ø–µ—Ä–≤—ã–µ —Å—Ç–æ–ª–∫–Ω—É–ª–∏—Å—å –≤ [–ì–ª–∞–≤–µ 1] (/course/chapter1).

<Youtube id="g8quOxoqhHQ"/>

–í –¥–∞–ª—å–Ω–µ–π—à–µ–º –æ–±—Å—É–∂–¥–µ–Ω–∏–∏ –º—ã –±—É–¥–µ–º —á–∞—Å—Ç–æ –ø—Ä–æ–≤–æ–¥–∏—Ç—å —Ä–∞–∑–ª–∏—á–∏–µ –º–µ–∂–¥—É "–º–µ–¥–ª–µ–Ω–Ω—ã–º–∏" –∏ "–±—ã—Å—Ç—Ä—ã–º–∏" —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞–º–∏. –ú–µ–¥–ª–µ–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã - —ç—Ç–æ —Ç–µ, —á—Ç–æ –Ω–∞–ø–∏—Å–∞–Ω—ã –Ω–∞ Python –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫–µ ü§ó Transformers, –∞ –±—ã—Å—Ç—Ä—ã–µ –≤–µ—Ä—Å–∏–∏ - —ç—Ç–æ —Ç–µ, —á—Ç–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç—Å—è –≤ ü§ó Tokenizers, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞–ø–∏—Å–∞–Ω—ã –Ω–∞ Rust. –ï—Å–ª–∏ –≤—ã –ø–æ–º–Ω–∏—Ç–µ —Ç–∞–±–ª–∏—Ü—É –∏–∑ [–ì–ª–∞–≤—ã 5](/course/chapter5/3), –≤ –∫–æ—Ç–æ—Ä–æ–π –ø—Ä–∏–≤–æ–¥–∏–ª–æ—Å—å, —Å–∫–æ–ª—å–∫–æ –≤—Ä–µ–º–µ–Ω–∏ –ø–æ—Ç—Ä–µ–±–æ–≤–∞–ª–æ—Å—å –±—ã—Å—Ç—Ä–æ–º—É –∏ –º–µ–¥–ª–µ–Ω–Ω–æ–º—É —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞–º –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ Drug Review Dataset, –≤—ã –¥–æ–ª–∂–Ω—ã –∏–º–µ—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ —Ç–æ–º, –ø–æ—á–µ–º—É –º—ã –Ω–∞–∑—ã–≤–∞–µ–º –∏—Ö –±—ã—Å—Ç—Ä—ã–º–∏ –∏ –º–µ–¥–ª–µ–Ω–Ω—ã–º–∏:

|               |  –ë—ã—Å—Ç—Ä—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä   | –ú–µ–¥–ª–µ–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
:--------------:|:----------------------:|:----------------------:
`batched=True`  | 10.8s                  | 4min41s
`batched=False` | 59.2s                  | 5min3s

<Tip warning={true}>

‚ö†Ô∏è –ö–æ–≥–¥–∞ –≤—ã —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç–µ –æ–¥–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ, –≤—ã –Ω–µ –≤—Å–µ–≥–¥–∞ —É–≤–∏–¥–∏—Ç–µ —Ä–∞–∑–Ω–∏—Ü—É –≤ —Å–∫–æ—Ä–æ—Å—Ç–∏ –º–µ–∂–¥—É –º–µ–¥–ª–µ–Ω–Ω–æ–π –∏ –±—ã—Å—Ç—Ä–æ–π –≤–µ—Ä—Å–∏—è–º–∏ –æ–¥–Ω–æ–≥–æ –∏ —Ç–æ–≥–æ –∂–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞. –ë–æ–ª–µ–µ —Ç–æ–≥–æ, –±—ã—Å—Ç—Ä–∞—è –≤–µ—Ä—Å–∏—è –º–æ–∂–µ—Ç –±—ã—Ç—å –¥–∞–∂–µ –º–µ–¥–ª–µ–Ω–Ω–µ–µ! –¢–æ–ª—å–∫–æ –ø—Ä–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –±–æ–ª—å—à–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–µ–∫—Å—Ç–æ–≤ –≤—ã —Å–º–æ–∂–µ—Ç–µ —É–≤–∏–¥–µ—Ç—å —Ä–∞–∑–Ω–∏—Ü—É.

</Tip>

## Batch encoding[[batch-encoding]]

<Youtube id="3umI3tm27Vw"/>

–†–µ–∑—É–ª—å—Ç–∞—Ç —Ä–∞–±–æ—Ç—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ - —ç—Ç–æ –Ω–µ –ø—Ä–æ—Å—Ç–æ–π —Å–ª–æ–≤–∞—Ä—å Python; —Ç–æ, —á—Ç–æ –º—ã –ø–æ–ª—É—á–∞–µ–º, - —ç—Ç–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –æ–±—ä–µ–∫—Ç `BatchEncoding`. –≠—Ç–æ –ø–æ–¥–∫–ª–∞—Å—Å —Å–ª–æ–≤–∞—Ä—è (–∏–º–µ–Ω–Ω–æ –ø–æ—ç—Ç–æ–º—É –º—ã —Ä–∞–Ω—å—à–µ –º–æ–≥–ª–∏ –±–µ–∑ –ø—Ä–æ–±–ª–µ–º –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç), –Ω–æ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–µ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –±—ã—Å—Ç—Ä—ã–º–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞–º–∏.

–ü–æ–º–∏–º–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —Ä–∞—Å–ø–∞—Ä–∞–ª–ª–µ–ª–∏–≤–∞–Ω–∏—è, –∫–ª—é—á–µ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–µ–π –±—ã—Å—Ç—Ä—ã—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ —è–≤–ª—è–µ—Ç—Å—è —Ç–æ, —á—Ç–æ –æ–Ω–∏ –≤—Å–µ–≥–¥–∞ –æ—Ç—Å–ª–µ–∂–∏–≤–∞—é—Ç –∏—Å—Ö–æ–¥–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω —Ç–µ–∫—Å—Ç–æ–≤, –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö –≤–∑—è—Ç—ã –∫–æ–Ω–µ—á–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã, - —ç—Ç—É —Ñ—É–Ω–∫—Ü–∏—é –º—ã –Ω–∞–∑—ã–≤–∞–µ–º *—Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å–º–µ—â–µ–Ω–∏–π (offset mapping)*. –≠—Ç–æ, –≤ —Å–≤–æ—é –æ—á–µ—Ä–µ–¥—å, –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç —Ç–∞–∫–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏, –∫–∞–∫ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ —Å –ø–æ—Ä–æ–∂–¥–µ–Ω–Ω—ã–º–∏ –∏–º —Ç–æ–∫–µ–Ω–∞–º–∏ –∏–ª–∏ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å —Ç–æ–∫–µ–Ω–æ–º, –≤ –∫–æ—Ç–æ—Ä–æ–º –æ–Ω –Ω–∞—Ö–æ–¥–∏—Ç—Å—è, –∏ –Ω–∞–æ–±–æ—Ä–æ—Ç.

–î–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –ø—Ä–∏–º–µ—Ä:

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
encoding = tokenizer(example)
print(type(encoding))
```

–ö–∞–∫ —É–ø–æ–º–∏–Ω–∞–ª–æ—Å—å —Ä–∞–Ω–µ–µ, –Ω–∞ –≤—ã—Ö–æ–¥–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –º—ã –ø–æ–ª—É—á–∞–µ–º –æ–±—ä–µ–∫—Ç `BatchEncoding`: 

```python out
<class 'transformers.tokenization_utils_base.BatchEncoding'>
```

–ü–æ—Å–∫–æ–ª—å–∫—É –∫–ª–∞—Å—Å `AutoTokenizer` –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤—ã–±–∏—Ä–∞–µ—Ç –±—ã—Å—Ç—Ä—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –æ–±—ä–µ–∫—Ç `BatchEncoding`. –£ –Ω–∞—Å –µ—Å—Ç—å –¥–≤–∞ —Å–ø–æ—Å–æ–±–∞ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –Ω–∞—à —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –±—ã—Å—Ç—Ä—ã–º –∏–ª–∏ –º–µ–¥–ª–µ–Ω–Ω—ã–º. –ú—ã –º–æ–∂–µ–º –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∞—Ç—Ä–∏–±—É—Ç `is_fast` —É `tokenizer`:

```python
tokenizer.is_fast
```

```python out
True
```

–∏–ª–∏ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ç–æ—Ç –∂–µ –∞—Ç—Ä–∏–±—É—Ç —É –æ–±—ä–µ–∫—Ç–∞ `encoding`:

```python
encoding.is_fast
```

```python out
True
```

–î–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º, —á—Ç–æ –Ω–∞–º –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–µ–ª–∞—Ç—å –±—ã—Å—Ç—Ä—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä. –í–æ-–ø–µ—Ä–≤—ã—Ö, –º—ã –º–æ–∂–µ–º –ø–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø –∫ —Ç–æ–∫–µ–Ω–∞–º –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –æ–±—Ä–∞—Ç–Ω–æ –≤ —Ç–æ–∫–µ–Ω—ã:

```py
encoding.tokens()
```

```python out
['[CLS]', 'My', 'name', 'is', 'S', '##yl', '##va', '##in', 'and', 'I', 'work', 'at', 'Hu', '##gging', 'Face', 'in',
 'Brooklyn', '.', '[SEP]']
```


–í —ç—Ç–æ–º —Å–ª—É—á–∞–µ —Ç–æ–∫–µ–Ω –≤ –∏–Ω–¥–µ–∫—Å–µ 5 ‚Äî —ç—Ç–æ `##yl`, –∫–æ—Ç–æ—Ä—ã–π —è–≤–ª—è–µ—Ç—Å—è —á–∞—Å—Ç—å—é —Å–ª–æ–≤–∞ ¬´Sylvain¬ª –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏. –ú—ã —Ç–∞–∫–∂–µ –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ—Ç–æ–¥ `word_ids()`, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –∏–Ω–¥–µ–∫—Å —Å–ª–æ–≤–∞, –∏–∑ –∫–æ—Ç–æ—Ä–æ–≥–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω:

```py
encoding.word_ids()
```

```python out
[None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None]
```


–ú—ã –º–æ–∂–µ–º –≤–∏–¥–µ—Ç—å, —á—Ç–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ `[CLS]` –∏ `[SEP]` —Å–æ–ø–æ—Å—Ç–∞–≤–ª—è—é—Ç—Å—è —Å `None`, –∞ –∑–∞—Ç–µ–º –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω —Å–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è —Å–æ —Å–ª–æ–≤–æ–º, –æ—Ç –∫–æ—Ç–æ—Ä–æ–≥–æ –æ–Ω –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç. –≠—Ç–æ –æ—Å–æ–±–µ–Ω–Ω–æ –ø–æ–ª–µ–∑–Ω–æ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–æ–≥–æ, –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ª–∏ —Ç–æ–∫–µ–Ω –≤ –Ω–∞—á–∞–ª–µ —Å–ª–æ–≤–∞ –∏–ª–∏ –¥–≤–∞ —Ç–æ–∫–µ–Ω–∞ –≤ –æ–¥–Ω–æ–º –∏ —Ç–æ–º –∂–µ —Å–ª–æ–≤–µ. –î–ª—è —ç—Ç–æ–≥–æ –º—ã –º–æ–≥–ª–∏ –±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ—Ñ–∏–∫—Å `##`, –Ω–æ –æ–Ω —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ —Ç–∏–ø–∞ BERT; —ç—Ç–æ—Ç –º–µ—Ç–æ–¥ —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è –ª—é–±–æ–≥–æ —Ç–∏–ø–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤, –ª–∏—à—å –±—ã –æ–Ω –±—ã–ª –±—ã—Å—Ç—Ä—ã–º. –í —Å–ª–µ–¥—É—é—â–µ–π –≥–ª–∞–≤–µ –º—ã —É–≤–∏–¥–∏–º, –∫–∞–∫ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç—É –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –º–µ—Ç–æ–∫, –∫–æ—Ç–æ—Ä—ã–µ –º—ã –∏–º–µ–µ–º –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞, –∫ —Ç–æ–∫–µ–Ω–∞–º –≤ —Ç–∞–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö, –∫–∞–∫ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π (NER) –∏ —Ç–µ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∞—Å—Ç–µ–π —Ä–µ—á–∏ (part-of-speech - POS). –ú—ã —Ç–∞–∫–∂–µ –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–µ –¥–ª—è –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Å–µ—Ö —Ç–æ–∫–µ–Ω–æ–≤, –ø—Ä–æ–∏—Å—Ö–æ–¥—è—â–∏—Ö –æ—Ç –æ–¥–Ω–æ–≥–æ –∏ —Ç–æ–≥–æ –∂–µ —Å–ª–æ–≤–∞, –ø—Ä–∏ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏ —è–∑—ã–∫–∞ –ø–æ –º–∞—Å–∫–µ (masked language modeling) (—ç—Ç–∞ —Ç–µ—Ö–Ω–∏–∫–∞ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è _–º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ–º –≤—Å–µ–≥–æ —Å–ª–æ–≤–∞ (whole word masking)_).

<Tip>

–ü–æ–Ω—è—Ç–∏–µ "—Å–ª–æ–≤–æ" –æ—á–µ–Ω—å —Å–ª–æ–∂–Ω–æ–µ. –ù–∞–ø—Ä–∏–º–µ—Ä, "I'll" (—Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ –æ—Ç "I will") —Å—á–∏—Ç–∞–µ—Ç—Å—è –æ–¥–Ω–∏–º –∏–ª–∏ –¥–≤—É–º—è —Å–ª–æ–≤–∞–º–∏? –ù–∞ —Å–∞–º–æ–º –¥–µ–ª–µ —ç—Ç–æ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –ø—Ä–∏–º–µ–Ω—è–µ–º–æ–π –∏–º –æ–ø–µ—Ä–∞—Ü–∏–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏. –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã –ø—Ä–æ—Å—Ç–æ —Ä–∞–∑–¥–µ–ª—è—é—Ç –ø—Ä–æ–±–µ–ª—ã, –ø–æ—ç—Ç–æ–º—É –æ–Ω–∏ –±—É–¥—É—Ç —Å—á–∏—Ç–∞—Ç—å —ç—Ç–æ –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º. –î—Ä—É–≥–∏–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –ø–æ–≤–µ—Ä—Ö –ø—Ä–æ–±–µ–ª–æ–≤, –ø–æ—ç—Ç–æ–º—É –±—É–¥—É—Ç —Å—á–∏—Ç–∞—Ç—å —ç—Ç–æ –¥–≤—É–º—è —Å–ª–æ–≤–∞–º–∏.

‚úèÔ∏è **–ü–æ–ø—Ä–æ–±—É–π—Ç–µ!** –°–æ–∑–¥–∞–π—Ç–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–∑ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã—Ö —Ç–æ—á–µ–∫ `bert-base-cased` –∏ `roberta-base` –∏ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–π—Ç–µ —Å –∏—Ö –ø–æ–º–æ—â—å—é "81s". –ß—Ç–æ –≤—ã –∑–∞–º–µ—Ç–∏–ª–∏? –ö–∞–∫–æ–≤—ã –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã —Å–ª–æ–≤?

</Tip>

–ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –º–µ—Ç–æ–¥ `sentence_ids()`, –∫–æ—Ç–æ—Ä—ã–π –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∞ —Å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º, –∏–∑ –∫–æ—Ç–æ—Ä–æ–≥–æ –æ–Ω–æ –≤–∑—è—Ç–æ (—Ö–æ—Ç—è –≤ —ç—Ç–æ–º —Å–ª—É—á–∞–µ —Ç—É –∂–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –º–æ–∂–µ—Ç –¥–∞—Ç—å –∏ `token_type_ids`, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º).

–ù–∞–∫–æ–Ω–µ—Ü, —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–æ–≤ `word_to_chars()` –∏–ª–∏ `token_to_chars()` –∏ `char_to_word()` –∏–ª–∏ `char_to_token()` –º—ã –º–æ–∂–µ–º —Å–æ–ø–æ—Å—Ç–∞–≤–∏—Ç—å –ª—é–±–æ–µ —Å–ª–æ–≤–æ –∏–ª–∏ —Ç–æ–∫–µ–Ω —Å —Å–∏–º–≤–æ–ª–∞–º–∏ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º —Ç–µ–∫—Å—Ç–µ –∏ –Ω–∞–æ–±–æ—Ä–æ—Ç. –ù–∞–ø—Ä–∏–º–µ—Ä, –º–µ—Ç–æ–¥ `word_ids()` —Å–æ–æ–±—â–∏–ª –Ω–∞–º, —á—Ç–æ `##yl` —è–≤–ª—è–µ—Ç—Å—è —á–∞—Å—Ç—å—é —Å–ª–æ–≤–∞ —Å –∏–Ω–¥–µ–∫—Å–æ–º 3, –Ω–æ –∫–∞–∫–æ–µ —ç—Ç–æ —Å–ª–æ–≤–æ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏? –ú—ã –º–æ–∂–µ–º –≤—ã—è—Å–Ω–∏—Ç—å —ç—Ç–æ —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º:


```py
start, end = encoding.word_to_chars(3)
example[start:end]
```

```python out
Sylvain
```

## `token-classification` –∫–∞–∫ —á–∞—Å—Ç—å –ø–∞–π–ø–ª–∞–π–Ω–∞ [[inside-the-token-classification-pipeline]]

–í [–ì–ª–∞–≤–µ 1](/course/ru/chapter1) –º—ã –≤–ø–µ—Ä–≤—ã–µ –ø–æ–ø—Ä–æ–±–æ–≤–∞–ª–∏ –ø—Ä–∏–º–µ–Ω–∏—Ç—å NER ‚Äî –≥–¥–µ –∑–∞–¥–∞—á–∞ —Å–æ—Å—Ç–æ–∏—Ç –≤ —Ç–æ–º, —á—Ç–æ–±—ã –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, –∫–∞–∫–∏–µ —á–∞—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —Ç–∞–∫–∏–º —Å—É—â–Ω–æ—Å—Ç—è–º, –∫–∞–∫ –ª—é–¥–∏, –º–µ—Å—Ç–∞ –∏–ª–∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ ‚Äî —Å –ø–∞–π–ø–ª–∞–π–Ω–æ–º ü§ó Transformers `pipeline()`. –ó–∞—Ç–µ–º, –≤ [–ì–ª–∞–≤–µ 2](/course/ru/chapter2), –º—ã —É–≤–∏–¥–µ–ª–∏, –∫–∞–∫ –∫–æ–Ω–≤–µ–π–µ—Ä –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ç—Ä–∏ —ç—Ç–∞–ø–∞, –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –∏–∑ –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è, –ø–µ—Ä–µ–¥–∞—á–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å –∏ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞. –ü–µ—Ä–≤—ã–µ –¥–≤–∞ —à–∞–≥–∞ –≤ –∫–æ–Ω–≤–µ–π–µ—Ä–µ `token-classification` —Ç–∞–∫–∏–µ –∂–µ, –∫–∞–∫ –∏ –≤ –ª—é–±–æ–º –¥—Ä—É–≥–æ–º –∫–æ–Ω–≤–µ–π–µ—Ä–µ, –Ω–æ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ–º–Ω–æ–≥–æ —Å–ª–æ–∂–Ω–µ–µ ‚Äî –¥–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º!
=======
–ö–∞–∫ –º—ã —É–∂–µ –≥–æ–≤–æ—Ä–∏–ª–∏, –≤—Å–µ —ç—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –±–ª–∞–≥–æ–¥–∞—Ä—è —Ç–æ–º—É, —á—Ç–æ –±—ã—Å—Ç—Ä—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç, –∏–∑ –∫–∞–∫–æ–≥–æ —É—á–∞—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω, –≤ —Å–ø–∏—Å–∫–µ *—Å–º–µ—â–µ–Ω–∏–π (offsets)*. –ß—Ç–æ–±—ã –ø—Ä–æ–∏–ª–ª—é—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å –∏—Ö –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ, –¥–∞–ª–µ–µ –º—ã –ø–æ–∫–∞–∂–µ–º, –∫–∞–∫ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–æ–Ω–≤–µ–π–µ—Ä–∞ `token-classification` –≤—Ä—É—á–Ω—É—é.

<Tip>

‚úèÔ∏è **–ü–æ–ø—Ä–æ–±—É–π—Ç–µ!** –°–æ–∑–¥–∞–π—Ç–µ —Å–≤–æ–π —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π –ø—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –∏ –ø–æ—Å–º–æ—Ç—Ä–∏—Ç–µ, —Å–º–æ–∂–µ—Ç–µ –ª–∏ –≤—ã –ø–æ–Ω—è—Ç—å, –∫–∞–∫–∏–µ —Ç–æ–∫–µ–Ω—ã —Å–≤—è–∑–∞–Ω—ã —Å –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞–º–∏ —Å–ª–æ–≤, –∞ —Ç–∞–∫–∂–µ –∫–∞–∫ –∏–∑–≤–ª–µ—á—å –¥–∏–∞–ø–∞–∑–æ–Ω—ã —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Å–ª–æ–≤–∞. –ß—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –±–æ–Ω—É—Å–Ω—ã–µ –æ—á–∫–∏, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–≤–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤ –∫–∞—á–µ—Å—Ç–≤–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –ø–æ—Å–º–æ—Ç—Ä–∏—Ç–µ, –±—É–¥—É—Ç –ª–∏ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∏–º–µ—Ç—å –¥–ª—è –≤–∞—Å —Å–º—ã—Å–ª.

</Tip>

## –í–Ω—É—Ç—Ä–∏ –∫–æ–Ω–≤–µ–π–µ—Ä–∞ `token-classification`[[inside-the-token-classification-pipeline]]

–í [–ì–ª–∞–≤–µ 1](/course/chapter1) –º—ã –≤–ø–µ—Ä–≤—ã–µ –ø–æ–ø—Ä–æ–±–æ–≤–∞–ª–∏ –ø—Ä–∏–º–µ–Ω–∏—Ç—å NER - –∫–æ–≥–¥–∞ –∑–∞–¥–∞—á–∞ —Å–æ—Å—Ç–æ–∏—Ç –≤ —Ç–æ–º, —á—Ç–æ–±—ã –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, –∫–∞–∫–∏–µ —á–∞—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —Å—É—â–Ω–æ—Å—Ç—è–º, —Ç–∞–∫–∏–º –∫–∞–∫ –ª—é–¥–∏, –º–µ—Å—Ç–∞ –∏–ª–∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ - —Å –ø–æ–º–æ—â—å—é —Ñ—É–Ω–∫—Ü–∏–∏ ü§ó Transformers `pipeline()`. –ó–∞—Ç–µ–º, –≤ [–ì–ª–∞–≤–µ 2](/course/chapter2), –º—ã —É–≤–∏–¥–µ–ª–∏, –∫–∞–∫ –∫–æ–Ω–≤–µ–π–µ—Ä –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ç—Ä–∏ —ç—Ç–∞–ø–∞, –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –∏–∑ –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é, –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å –∏ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫—É. –ü–µ—Ä–≤—ã–µ –¥–≤–∞ —à–∞–≥–∞ –≤ –∫–æ–Ω–≤–µ–π–µ—Ä–µ `token-classification` —Ç–∞–∫–∏–µ –∂–µ, –∫–∞–∫ –∏ –≤ –ª—é–±–æ–º –¥—Ä—É–≥–æ–º –∫–æ–Ω–≤–µ–π–µ—Ä–µ, –Ω–æ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ–º–Ω–æ–≥–æ —Å–ª–æ–∂–Ω–µ–µ - –¥–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫ —ç—Ç–æ —Å–¥–µ–ª–∞—Ç—å!


{#if fw === 'pt'}

<Youtube id="0E7ltQB7fM8"/>

{:else}

<Youtube id="PrX4CjrVnNc"/>

{/if}

### –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Å –ø–æ–º–æ—â—å—é –ø–∞–π–ø–ª–∞–π–Ω–∞[[getting-the-base-results-with-the-pipeline]]

–í–æ-–ø–µ—Ä–≤—ã—Ö, –¥–∞–≤–∞–π—Ç–µ –≤–æ–∑—å–º–µ–º –ø–∞–π–ø–ª–∞–π–Ω –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤, —á—Ç–æ–±—ã –º—ã –º–æ–≥–ª–∏ –ø–æ–ª—É—á–∏—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–æ–¥–µ–ª—å [`dbmdz/bert-large-cased-finetuned-conll03-english`](https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english); –æ–Ω–∞ —Ä–µ—à–∞–µ—Ç –∑–∞–¥–∞—á—É NER –¥–ª—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π:

```py
from transformers import pipeline

token_classifier = pipeline("token-classification")
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python out
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

–ú–æ–¥–µ–ª—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–ª–∞ –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–≤ "Sylvain", –∫–∞–∫ —á–µ–ª–æ–≤–µ–∫–∞, –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π "Hugging Face", –∫–∞–∫ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é, –∞ —Ç–æ–∫–µ–Ω "Brooklyn" - –∫–∞–∫ –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ. –ú—ã —Ç–∞–∫–∂–µ –º–æ–∂–µ–º –ø–æ–ø—Ä–æ—Å–∏—Ç—å –∫–æ–Ω–≤–µ–π–µ—Ä —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–∫–µ–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –æ–¥–Ω–æ–π –∏ —Ç–æ–π –∂–µ —Å—É—â–Ω–æ—Å—Ç–∏:

```py
from transformers import pipeline

token_classifier = pipeline("token-classification", aggregation_strategy="simple")
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python out
[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```


–¢–µ–ø–µ—Ä—å –¥–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫ –ø–æ–ª—É—á–∏—Ç—å —ç—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ `pipeline()`!


### –û—Ç –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∫ –ø—Ä–æ–≥–Ω–æ–∑–∞–º[[from-inputs-to-predictions]]

{#if fw === 'pt'}

–°–Ω–∞—á–∞–ª–∞ –Ω–∞–º –Ω—É–∂–Ω–æ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å –Ω–∞—à –≤–≤–æ–¥ –∏ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å –µ–≥–æ —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å. –≠—Ç–æ –¥–µ–ª–∞–µ—Ç—Å—è —Ç–æ—á–Ω–æ —Ç–∞–∫ –∂–µ, –∫–∞–∫ –≤ [–ì–ª–∞–≤–µ 2](/course/chapter2); –º—ã –∏–Ω—Å—Ç–∞–Ω—Ü–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –º–æ–¥–µ–ª—å —Å –ø–æ–º–æ—â—å—é –∫–ª–∞—Å—Å–æ–≤ `AutoXxx`, –∞ –∑–∞—Ç–µ–º –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö –≤ –Ω–∞—à–µ–º –ø—Ä–∏–º–µ—Ä–µ:


```py
from transformers import AutoTokenizer, AutoModelForTokenClassification

model_checkpoint = "dbmdz/bert-large-cased-finetuned-conll03-english"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)

example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
inputs = tokenizer(example, return_tensors="pt")
outputs = model(**inputs)
```


–ü–æ—Å–∫–æ–ª—å–∫—É –∑–¥–µ—Å—å –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º `AutoModelForTokenClassification`, –º—ã –ø–æ–ª—É—á–∞–µ–º –æ–¥–∏–Ω –Ω–∞–±–æ—Ä –ª–æ–≥–∏—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –≤–æ –≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:

```py
print(inputs["input_ids"].shape)
print(outputs.logits.shape)
```

```python out
torch.Size([1, 19])
torch.Size([1, 19, 9])
```

{:else}


–°–Ω–∞—á–∞–ª–∞ –Ω–∞–º –Ω—É–∂–Ω–æ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å –Ω–∞—à –≤–≤–æ–¥ –∏ –ø–µ—Ä–µ–¥–∞—Ç—å –µ–≥–æ —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å. –≠—Ç–æ –¥–µ–ª–∞–µ—Ç—Å—è —Ç–æ—á–Ω–æ —Ç–∞–∫ –∂–µ, –∫–∞–∫ –≤ [–ì–ª–∞–≤–µ 2](/course/ru/chapter2); –º—ã —Å–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏, –∏—Å–ø–æ–ª—å–∑—É—è –∫–ª–∞—Å—Å—ã `TFAutoXxx`, –∞ –∑–∞—Ç–µ–º –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö –≤ –Ω–∞—à–µ–º –ø—Ä–∏–º–µ—Ä–µ:

```py
from transformers import AutoTokenizer, TFAutoModelForTokenClassification

model_checkpoint = "dbmdz/bert-large-cased-finetuned-conll03-english"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = TFAutoModelForTokenClassification.from_pretrained(model_checkpoint)

example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
inputs = tokenizer(example, return_tensors="tf")
outputs = model(**inputs)
```

–ü–æ—Å–∫–æ–ª—å–∫—É –∑–¥–µ—Å—å –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º `TFAutoModelForTokenClassification` here, –º—ã –ø–æ–ª—É—á–∞–µ–º –æ–¥–∏–Ω –Ω–∞–±–æ—Ä –ª–æ–≥–∏—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –≤–æ –≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:

```py
print(inputs["input_ids"].shape)
print(outputs.logits.shape)
```

```python out
(1, 19)
(1, 19, 9)
```

{/if}


–£ –Ω–∞—Å –µ—Å—Ç—å –±–∞—Ç—á —Å 1 –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –∏–∑ 19 —Ç–æ–∫–µ–Ω–æ–≤, –∏ –º–æ–¥–µ–ª—å –∏–º–µ–µ—Ç 9 —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Ç–æ–∫, –ø–æ—ç—Ç–æ–º—É –≤—ã—Ö–æ–¥ –º–æ–¥–µ–ª–∏ –∏–º–µ–µ—Ç —Ñ–æ—Ä–º—É 1 x 19 x 9. –ö–∞–∫ –∏ –¥–ª—è –∫–æ–Ω–≤–µ–π–µ—Ä–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞, –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é softmax –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —ç—Ç–∏—Ö –ª–æ–≥–∏—Ç–æ–≤ –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∏ –±–µ—Ä–µ–º argmax –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ (–æ–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –º—ã –º–æ–∂–µ–º –≤–∑—è—Ç—å argmax –¥–ª—è –ª–æ–≥–∏—Ç–æ–≤, –ø–æ—Ç–æ–º—É —á—Ç–æ softmax –Ω–µ –º–µ–Ω—è–µ—Ç –ø–æ—Ä—è–¥–æ–∫):


{#if fw === 'pt'}

```py
import torch

probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()
predictions = outputs.logits.argmax(dim=-1)[0].tolist()
print(predictions)
```

{:else}

```py
import tensorflow as tf

probabilities = tf.math.softmax(outputs.logits, axis=-1)[0]
probabilities = probabilities.numpy().tolist()
predictions = tf.math.argmax(outputs.logits, axis=-1)[0]
predictions = predictions.numpy().tolist()
print(predictions)
```

{/if}

```python out
[0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0]
```

–ê—Ç—Ä–∏–±—É—Ç `model.config.id2label` —Å–æ–¥–µ—Ä–∂–∏–∏—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –∏–Ω–¥–µ–∫—Å–æ–≤ –∏ –ª–µ–π–±–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:

```py
model.config.id2label
```

```python out
{0: 'O',
 1: 'B-MISC',
 2: 'I-MISC',
 3: 'B-PER',
 4: 'I-PER',
 5: 'B-ORG',
 6: 'I-ORG',
 7: 'B-LOC',
 8: 'I-LOC'}
```


–ö–∞–∫ –º—ã –≤–∏–¥–µ–ª–∏ —Ä–∞–Ω–µ–µ, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç 9 –º–µ—Ç–æ–∫: `O` - —ç—Ç–æ –º–µ—Ç–∫–∞ –¥–ª—è —Ç–æ–∫–µ–Ω–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –≤—Ö–æ–¥—è—Ç –Ω–∏ –≤ –æ–¥–Ω—É –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—É—é —Å—É—â–Ω–æ—Å—Ç—å (–æ–Ω–∞ –æ–∑–Ω–∞—á–∞–µ—Ç "–≤–Ω–µ"), –∞ –∑–∞—Ç–µ–º —É –Ω–∞—Å –µ—Å—Ç—å –¥–≤–µ –º–µ—Ç–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞ —Å—É—â–Ω–æ—Å—Ç–∏ (miscellaneous, person, organization –∏ location). –ú–µ—Ç–∫–∞ `B-XXX` —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —Ç–æ, —á—Ç–æ —Ç–æ–∫–µ–Ω –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –Ω–∞—á–∞–ª–µ —Å—É—â–Ω–æ—Å—Ç–∏ `XXX`, –∞ –º–µ—Ç–∫–∞ `I-XXX` —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —Ç–æ, —á—Ç–æ —Ç–æ–∫–µ–Ω –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ —Å—É—â–Ω–æ—Å—Ç–∏ `XXX`. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –≤ –¥–∞–Ω–Ω–æ–º –ø—Ä–∏–º–µ—Ä–µ –º—ã –æ–∂–∏–¥–∞–µ–º, —á—Ç–æ –Ω–∞—à–∞ –º–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Ç–æ–∫–µ–Ω `S` –∫–∞–∫ `B-PER` (–Ω–∞—á–∞–ª–æ —Å—É—â–Ω–æ—Å—Ç–∏ person), –∞ —Ç–æ–∫–µ–Ω—ã `##yl`, `##va` –∏ `##in` –∫–∞–∫ `I-PER` (–≤–Ω—É—Ç—Ä–∏ —Å—É—â–Ω–æ—Å—Ç–∏ person). 

–í—ã –º–æ–∂–µ—Ç–µ –ø–æ–¥—É–º–∞—Ç—å, —á—Ç–æ –º–æ–¥–µ–ª—å –≤ –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ –æ—à–∏–±–ª–∞—Å—å, –ø–æ—Å–∫–æ–ª—å–∫—É –ø—Ä–∏—Å–≤–æ–∏–ª–∞ –≤—Å–µ–º —á–µ—Ç—ã—Ä–µ–º —Ç–æ–∫–µ–Ω–∞–º –º–µ—Ç–∫—É `I-PER`, –Ω–æ —ç—Ç–æ –Ω–µ —Å–æ–≤—Å–µ–º —Ç–∞–∫. –ù–∞ —Å–∞–º–æ–º –¥–µ–ª–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –¥–≤–∞ —Ñ–æ—Ä–º–∞—Ç–∞ –¥–ª—è –º–µ—Ç–æ–∫ `B-` –∏ `I-`: *IOB1* –∏ *IOB2*. –§–æ—Ä–º–∞—Ç IOB2 (—Ä–æ–∑–æ–≤—ã–π —Ü–≤–µ—Ç –Ω–∏–∂–µ) - —ç—Ç–æ —Ç–æ—Ç, –∫–æ—Ç–æ—Ä—ã–π –º—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –≤ —Ñ–æ—Ä–º–∞—Ç–µ IOB1 (—Å–∏–Ω–∏–π —Ü–≤–µ—Ç) –º–µ—Ç–∫–∏, –Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è —Å `B-`, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –¥–≤—É—Ö —Å–æ—Å–µ–¥–Ω–∏—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π –æ–¥–Ω–æ–≥–æ —Ç–∏–ø–∞. –ò—Å–ø–æ–ª—å–∑—É–µ–º–∞—è –Ω–∞–º–∏ –º–æ–¥–µ–ª—å –±—ã–ª–∞ –¥–æ–æ–±—É—á–µ–Ω–∞ –Ω–∞ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑—É—é—â–µ–º —ç—Ç–æ—Ç —Ñ–æ—Ä–º–∞—Ç, –ø–æ—ç—Ç–æ–º—É –æ–Ω–∞ –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ—Ç —Ç–æ–∫–µ–Ω—É `S` –º–µ—Ç–∫—É `I-PER`.


<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions.svg" alt="IOB1 vs IOB2 format"/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions-dark.svg" alt="IOB1 vs IOB2 format"/>
</div>

–° –ø–æ–º–æ—â—å—é —ç—Ç–æ–≥–æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –º—ã –≥–æ—Ç–æ–≤—ã –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ—Å—Ç–∏ (–ø–æ—á—Ç–∏ –ø–æ–ª–Ω–æ—Å—Ç—å—é) —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–µ—Ä–≤–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ ‚Äî –º—ã –º–æ–∂–µ–º –ø—Ä–æ—Å—Ç–æ –ø–æ–ª—É—á–∏—Ç—å –æ—Ü–µ–Ω–∫—É –∏ –º–µ—Ç–∫—É –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞, –∫–æ—Ç–æ—Ä—ã–π –Ω–µ –±—ã–ª –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω –∫–∞–∫ `O`:

```py
results = []
tokens = inputs.tokens()

for idx, pred in enumerate(predictions):
    label = model.config.id2label[pred]
    if label != "O":
        results.append(
            {"entity": label, "score": probabilities[idx][pred], "word": tokens[idx]}
        )

print(results)
```

```python out
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S'},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl'},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va'},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in'},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu'},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging'},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face'},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn'}]
```

–≠—Ç–æ –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–µ –Ω–∞ —Ç–æ, —á—Ç–æ —É –Ω–∞—Å –±—ã–ª–æ —Ä–∞–Ω—å—à–µ, –∑–∞ –æ–¥–Ω–∏–º –∏—Å–∫–ª—é—á–µ–Ω–∏–µ–º: –∫–æ–Ω–≤–µ–π–µ—Ä —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏–ª –Ω–∞–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ `start` –∏ `end` –∫–∞–∂–¥–æ–π —Å—É—â–Ω–æ—Å—Ç–∏ –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏. –í–æ—Ç —Ç—É—Ç-—Ç–æ –∏ –ø—Ä–∏–≥–æ–¥–∏—Ç—Å—è –Ω–∞—à–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å–º–µ—â–µ–Ω–∏–π. –ß—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å —Å–º–µ—â–µ–Ω–∏—è, –Ω–∞–º –Ω—É–∂–Ω–æ –ø—Ä–æ—Å—Ç–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å `return_offsets_mapping=True`, –∫–æ–≥–¥–∞ –º—ã –ø—Ä–∏–º–µ–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∫ –Ω–∞—à–∏–º –≤—Ö–æ–¥–Ω—ã–º –¥–∞–Ω–Ω—ã–º:

```py
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
inputs_with_offsets["offset_mapping"]
```

```python out
[(0, 0), (0, 2), (3, 7), (8, 10), (11, 12), (12, 14), (14, 16), (16, 18), (19, 22), (23, 24), (25, 29), (30, 32),
 (33, 35), (35, 40), (41, 45), (46, 48), (49, 57), (57, 58), (0, 0)]
```

–ö–∞–∂–¥—ã–π –∫–æ—Ä—Ç–µ–∂ - —ç—Ç–æ —É—á–∞—Å—Ç–æ–∫ —Ç–µ–∫—Å—Ç–∞, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –∫–∞–∂–¥–æ–º—É —Ç–æ–∫–µ–Ω—É, –≥–¥–µ `(0, 0)` –∑–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤. –ú—ã —É–∂–µ –≤–∏–¥–µ–ª–∏, —á—Ç–æ —Ç–æ–∫–µ–Ω —Å –∏–Ω–¥–µ–∫—Å–æ–º 5 - —ç—Ç–æ `##yl`, –∫–æ—Ç–æ—Ä—ã–π –∏–º–µ–µ—Ç `(12, 14)` –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Å–º–µ—â–µ–Ω–∏—è. –ï—Å–ª–∏ –º—ã –≤–æ–∑—å–º–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç –≤ –Ω–∞—à–µ–º –ø—Ä–∏–º–µ—Ä–µ:

```py
example[12:14]
```

–º—ã –ø–æ–ª—É—á–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω —Ç–µ–∫—Å—Ç–∞ –±–µ–∑ `##`:

```python out
yl
```

–ò—Å–ø–æ–ª—å–∑—É—è —ç—Ç–æ, –º—ã —Ç–µ–ø–µ—Ä—å –º–æ–∂–µ–º –∑–∞–≤–µ—Ä—à–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç—å –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:

```py
results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets["offset_mapping"]

for idx, pred in enumerate(predictions):
    label = model.config.id2label[pred]
    if label != "O":
        start, end = offsets[idx]
        results.append(
            {
                "entity": label,
                "score": probabilities[idx][pred],
                "word": tokens[idx],
                "start": start,
                "end": end,
            }
        )

print(results)
```

```python out
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

–≠—Ç–æ —Ç–æ –∂–µ —Å–∞–º–æ–µ, —á—Ç–æ –º—ã –ø–æ–ª—É—á–∏–ª–∏ –æ—Ç –ø–µ—Ä–≤–æ–≥–æ –∫–æ–Ω–≤–µ–π–µ—Ä–∞!

### –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Å—É—â–Ω–æ—Å—Ç–µ–π[[grouping-entities]]

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å–º–µ—â–µ–Ω–∏–π –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ –∏ –∫–æ–Ω–µ—á–Ω–æ–≥–æ –∫–ª—é—á–µ–π –¥–ª—è –∫–∞–∂–¥–æ–π —Å—É—â–Ω–æ—Å—Ç–∏ —É–¥–æ–±–Ω–æ, –Ω–æ —ç—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Å—Ç—Ä–æ–≥–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–π. –û–¥–Ω–∞–∫–æ –∫–æ–≥–¥–∞ –º—ã –∑–∞—Ö–æ—Ç–∏–º —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å —Å—É—â–Ω–æ—Å—Ç–∏ –≤–º–µ—Å—Ç–µ, —Å–º–µ—â–µ–Ω–∏—è –∏–∑–±–∞–≤—è—Ç –Ω–∞—Å –æ—Ç –±–æ–ª—å—à–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –±–µ—Å–ø–æ—Ä—è–¥–æ—á–Ω–æ–≥–æ –∫–æ–¥–∞. –ù–∞–ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ –±—ã –º—ã —Ö–æ—Ç–µ–ª–∏ —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–∫–µ–Ω—ã `Hu`, `##gging` –∏ `Face`, –º—ã –º–æ–≥–ª–∏ –±—ã —Å–æ–∑–¥–∞—Ç—å —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞, —Å–æ–≥–ª–∞—Å–Ω–æ –∫–æ—Ç–æ—Ä—ã–º –ø–µ—Ä–≤—ã–µ –¥–≤–∞ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø—Ä–∏—Å–æ–µ–¥–∏–Ω–µ–Ω—ã, —É–¥–∞–ª–∏–≤ `##`, –∞ `Face` –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –¥–æ–±–∞–≤–ª–µ–Ω —á–µ—Ä–µ–∑ –ø—Ä–æ–±–µ–ª, –ø–æ—Å–∫–æ–ª—å–∫—É –æ–Ω –Ω–µ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å `##` - –Ω–æ —ç—Ç–æ –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Ç–æ–ª—å–∫–æ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ç–∏–ø–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞. –î–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ SentencePiece –∏–ª–∏ Byte-Pair-Encoding –Ω–∞–º –ø—Ä–∏–¥–µ—Ç—Å—è –Ω–∞–ø–∏—Å–∞—Ç—å –¥—Ä—É–≥–æ–π –Ω–∞–±–æ—Ä –ø—Ä–∞–≤–∏–ª (–æ –Ω–∏—Ö –º—ã –ø–æ–≥–æ–≤–æ—Ä–∏–º –ø–æ–∑–∂–µ –≤ —ç—Ç–æ–π –≥–ª–∞–≤–µ).

–° –ø–æ–º–æ—â—å—é —Å–º–µ—â–µ–Ω–∏–π –≤–µ—Å—å —ç—Ç–æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∫–æ–¥ –æ—Ç–ø–∞–¥–∞–µ—Ç: –º—ã –ø—Ä–æ—Å—Ç–æ –º–æ–∂–µ–º –≤–∑—è—Ç—å –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Ç–µ–∫—Å—Ç–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ–∫, –∫–æ—Ç–æ—Ä—ã–π –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –ø–µ—Ä–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –∏ –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –ø–æ—Å–ª–µ–¥–Ω–∏–º. –¢–∞–∫, –≤ —Å–ª—É—á–∞–µ —Å —Ç–æ–∫–µ–Ω–∞–º–∏ `Hu`, `##gging` –∏ `Face` –º—ã –¥–æ–ª–∂–Ω—ã –Ω–∞—á–∞—Ç—å —Å —Å–∏–º–≤–æ–ª–∞ 33 (–Ω–∞—á–∞–ª–æ `Hu`) –∏ –∑–∞–∫–æ–Ω—á–∏—Ç—å —Å–∏–º–≤–æ–ª–æ–º 45 (–∫–æ–Ω–µ—Ü `Face`):


```py
example[33:45]
```

```python out
Hugging Face
```

–ß—Ç–æ–±—ã –Ω–∞–ø–∏—Å–∞—Ç—å –∫–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –≤—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫—É –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –ø—Ä–∏ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–µ –æ–±—ä–µ–∫—Ç–æ–≤, –º—ã —Å–≥—Ä—É–ø–ø–∏—Ä—É–µ–º –æ–±—ä–µ–∫—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ —Å–ª–µ–¥—É—é—Ç –¥—Ä—É–≥ –∑–∞ –¥—Ä—É–≥–æ–º –∏ –ø–æ–º–µ—á–µ–Ω—ã `I-XXX`, –∑–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ–º –ø–µ—Ä–≤–æ–≥–æ, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ–º–µ—á–µ–Ω –∫–∞–∫ `B-XXX` –∏–ª–∏ `I-XXX` (—Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –º—ã –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å —Å—É—â–Ω–æ—Å—Ç—å, –∫–æ–≥–¥–∞ –ø–æ–ª—É—á–∞–µ–º `O`, –Ω–æ–≤—ã–π —Ç–∏–ø —Å—É—â–Ω–æ—Å—Ç–∏ –∏–ª–∏ `B-XXX`, –∫–æ—Ç–æ—Ä—ã–µ –≥–æ–≤–æ—Ä—è—Ç –Ω–∞–º, —á—Ç–æ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å—É—â–Ω–æ—Å—Ç—å —Ç–æ–≥–æ –∂–µ —Ç–∏–ø–∞):

```py
import numpy as np

results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets["offset_mapping"]

idx = 0
while idx < len(predictions):
    pred = predictions[idx]
    label = model.config.id2label[pred]
    if label != "O":

        # –£–¥–∞–ª–∏–º B- –∏–ª–∏ I-
        label = label[2:]
        start, _ = offsets[idx]

        # –°–æ–±–µ—Ä—ë–º –≤—Å–µ —Ç–æ–∫–µ–Ω—ã, –ø–æ–º–µ—á–µ–Ω–Ω—ã–µ I-–º–µ—Ç–∫–æ–π

        all_scores = []
        while (
            idx < len(predictions)
            and model.config.id2label[predictions[idx]] == f"I-{label}"
        ):
            all_scores.append(probabilities[idx][pred])
            _, end = offsets[idx]
            idx += 1


        # –û—Ü–µ–Ω–∫–∞ —è–≤–ª—è–µ—Ç—Å—è —Å—Ä–µ–¥–Ω–∏–º –∑–Ω–∞—á–µ–Ω–∏–µ–º –≤—Å–µ—Ö –æ—Ü–µ–Ω–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —ç—Ç–æ–π —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—É—â–Ω–æ—Å—Ç–∏

        score = np.mean(all_scores).item()
        word = example[start:end]
        results.append(
            {
                "entity_group": label,
                "score": score,
                "word": word,
                "start": start,
                "end": end,
            }
        )
    idx += 1

print(results)
```


–ò –º—ã –ø–æ–ª—É—á–∏–ª–∏ —Ç–∞–∫–æ–π –∂–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –∫–∞–∫ –∏ –≤–æ –≤—Ç–æ—Ä–æ–º –ø–∞–π–ø–ª–∞–π–Ω–µ!

```python out
[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

–ï—â–µ –æ–¥–∏–Ω –ø—Ä–∏–º–µ—Ä –∑–∞–¥–∞—á–∏, –≤ –∫–æ—Ç–æ—Ä–æ–π —ç—Ç–∏ —Å–º–µ—â–µ–Ω–∏—è —á—Ä–µ–∑–≤—ã—á–∞–π–Ω–æ –ø–æ–ª–µ–∑–Ω—ã, - question answering. –ü–æ–≥—Ä—É–∂–µ–Ω–∏–µ –≤ —ç—Ç–æ—Ç –∫–æ–Ω–≤–µ–π–µ—Ä, –∫–æ—Ç–æ—Ä–æ–µ –º—ã —Å–¥–µ–ª–∞–µ–º –≤ —Å–ª–µ–¥—É—é—â–µ–º —Ä–∞–∑–¥–µ–ª–µ, —Ç–∞–∫–∂–µ –ø–æ–∑–≤–æ–ª–∏—Ç –Ω–∞–º –≤–∑–≥–ª—è–Ω—É—Ç—å –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω—é—é –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫–µ ü§ó Transformers: —Ä–∞–±–æ—Ç–∞ —Å –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–Ω—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏ (overflowing tokens), –∫–æ–≥–¥–∞ –º—ã —É—Å–µ–∫–∞–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–æ –∑–∞–¥–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã.
