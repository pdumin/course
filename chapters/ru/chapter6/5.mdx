# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é Byte-Pair Encoding[[byte-pair-encoding-tokenization]]

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section5.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section5.ipynb"},
]} />

–ê–ª–≥–æ—Ä–∏—Ç–º BPE –ø–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω–æ –±—ã–ª —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω –¥–ª—è —Å–∂–∞—Ç–∏—è —Ç–µ–∫—Å—Ç–æ–≤, –∞ –∑–∞—Ç–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è OpenAI –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –ø—Ä–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –º–æ–¥–µ–ª–∏ GPT. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–Ω–æ–≥–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤, –≤–∫–ª—é—á–∞—è GPT, GPT-2, Roberta, Bart –∏ Deberta.

<Youtube id="HEikzVL-lZU"/>

<Tip>

üí° –≠—Ç–æ—Ç —Ä–∞–∑–¥–µ–ª –ø–æ–¥—Ä–æ–±–Ω–æ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç BPE –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –µ–≥–æ –ø–æ–ª–Ω—É—é —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é. –í—ã –º–æ–∂–µ—Ç–µ –ø–µ—Ä–µ–π—Ç–∏ –≤ –∫–æ–Ω–µ—Ü —ç—Ç–æ–π —Å—Ç–∞—Ç—å–∏, –µ—Å–ª–∏ –≤–∞–º –Ω–µ–æ–±—Ö–æ–¥–∏–º —Ç–æ–ª—å–∫–æ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω—ã–π –æ–±–∑–æ—Ä –∞–ª–≥–æ—Ä–∏—Ç–º–∞.

</Tip>

## –ê–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è[[training-algorithm]]

–û–±—É—á–µ–Ω–∏–µ BPE –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ —Å–ª–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –≤ –∫–æ—Ä–ø—É—Å–µ (–ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —ç—Ç–∞–ø–æ–≤ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏). –í –∫–∞—á–µ—Å—Ç–≤–µ –æ—á–µ–Ω—å –ø—Ä–æ—Å—Ç–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º —Å–ª–µ–¥—É—é—â–∏–π: –Ω–∞—à –∫–æ—Ä–ø—É—Å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —ç—Ç–∏ –ø—è—Ç—å —Å–ª–æ–≤: 

```
"hug", "pug", "pun", "bun", "hugs"
```

–ë–∞–∑–æ–≤—ã–π —Å–ª–æ–≤–∞—Ä—å –≤ —Ç–∞–∫–æ–º —Å–ª—É—á–∞–µ –±—É–¥–µ—Ç —Ç–∞–∫–∏–º: `["b", "g", "h", "n", "p", "s", "u"]`. –î–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤ —ç—Ç–æ—Ç –±–∞–∑–æ–≤—ã–π —Å–ª–æ–≤–∞—Ä—å –±—É–¥–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–∞–∫ –º–∏–Ω–∏–º—É–º –≤—Å–µ —Å–∏–º–≤–æ–ª—ã ASCII, –∏, –≤–µ—Ä–æ—è—Ç–Ω–æ, —Ç–∞–∫–∂–µ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Å–∏–º–≤–æ–ª—ã Unicode. –ï—Å–ª–∏ –ø—Ä–∏–º–µ—Ä —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–∏–º–≤–æ–ª, –∫–æ—Ç–æ—Ä–æ–≥–æ –Ω–µ—Ç –≤ —É—á–µ–±–Ω–æ–º –∫–æ—Ä–ø—É—Å–µ, —ç—Ç–æ—Ç —Å–∏–º–≤–æ–ª –±—É–¥–µ—Ç –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω –≤ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–æ–∫–µ–Ω. –≠—Ç–æ –æ–¥–Ω–∞ –∏–∑ –ø—Ä–∏—á–∏–Ω, –ø–æ—á–µ–º—É –º–Ω–æ–≥–∏–µ –º–æ–¥–µ–ª–∏ –ù–õ–ü –æ—á–µ–Ω—å –ø–ª–æ—Ö–æ –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –∫–æ–Ω—Ç–µ–Ω—Ç —Å –ø–æ–º–æ—â—å—é —ç–º–æ–¥–∑–∏.

<Tip>

–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã GPT-2 –∏ RoBERTa (–∫–æ—Ç–æ—Ä—ã–µ –¥–æ–≤–æ–ª—å–Ω–æ –ø–æ—Ö–æ–∂–∏) –∏–º–µ—é—Ç —É–º–Ω—ã–π —Å–ø–æ—Å–æ–± —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è —Å —ç—Ç–∏–º: –æ–Ω–∏ —Å–º–æ—Ç—Ä—è—Ç –Ω–∞ —Å–ª–æ–≤–∞ –Ω–µ –∫–∞–∫ –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å Unicode-—Å–∏–º–≤–æ–ª–æ–≤, –∞ –∫–∞–∫ –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –±–∞–π—Ç–æ–≤. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –±–∞–∑–æ–≤—ã–π —Å–ª–æ–≤–∞—Ä—å –∏–º–µ–µ—Ç –Ω–µ–±–æ–ª—å—à–æ–π —Ä–∞–∑–º–µ—Ä (256), –Ω–æ –±—É–¥–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –≤—Å–µ —Å–∏–º–≤–æ–ª—ã –∏ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤ –≤ –Ω–µ–º –Ω–µ –±—É–¥–µ—Ç. –≠—Ç–æ—Ç —Ç—Ä—é–∫ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è *byte-level BPE*.

</Tip>

–ü–æ–ª—É—á–∏–≤ —ç—Ç–æ—Ç –±–∞–∑–æ–≤—ã–π —Å–ª–æ–≤–∞—Ä—å, –º—ã –¥–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã –¥–æ —Ç–µ—Ö –ø–æ—Ä, –ø–æ–∫–∞ –Ω–µ –±—É–¥–µ—Ç –¥–æ—Å—Ç–∏–≥–Ω—É—Ç –Ω—É–∂–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è. –≠—Ç–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è *—Å–ª–∏—è–Ω–∏—è–º*: –∫–æ—Ç–æ—Ä—ã–µ —è–≤–ª—è—é—Ç—Å—è –ø—Ä–∞–≤–∏–ª–∞–º–∏ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –¥–≤—É—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Å–ª–æ–≤–∞—Ä—è –≤ –Ω–æ–≤—ã–π. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –≤ –Ω–∞—á–∞–ª–µ —ç—Ç–∏ —Å–ª–∏—è–Ω–∏—è –±—É–¥—É—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å —Ç–æ–∫–µ–Ω—ã —Å –¥–≤—É–º—è —Å–∏–º–≤–æ–ª–∞–º–∏, –∞ –∑–∞—Ç–µ–º, –ø–æ –º–µ—Ä–µ –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è, –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–µ –ø–æ–¥—Å—Ç—Ä–æ–∫–∏.

–ù–∞ –ª—é–±–æ–º —à–∞–≥–µ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∞–ª–≥–æ—Ä–∏—Ç–º BPE –±—É–¥–µ—Ç –∏—Å–∫–∞—Ç—å –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—É—é –ø–∞—Ä—É —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤ (–ø–æ–¥ ¬´–ø–∞—Ä–æ–π¬ª, –∑–¥–µ—Å—å –º—ã –∏–º–µ–µ–º –≤ –≤–∏–¥—É –¥–≤–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∞ –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º). –≠—Ç–∞ –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç–∞—è –ø–∞—Ä–∞ - —Ç–∞, –∫–æ—Ç–æ—Ä–∞—è –±—É–¥–µ—Ç –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∞, —Ç–∞–∫ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ —Å–ª–∏–≤–∞—é—Ç—Å—è –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã–µ –ø–∞—Ä—ã —Ç–æ–∫–µ–Ω–æ–≤. 

–í–µ—Ä–Ω–µ–º—Å—è –∫ –Ω–∞—à–µ–º—É –ø—Ä–∏–º–µ—Ä—É –≤—ã—à–µ –∏ –ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —á—Ç–æ —á–∞—Å—Ç–æ—Ç—ã –Ω–∞—à–∏—Ö —Å–ª–æ–≤ —Å–ª–µ–¥—É—é—â–∏–µ: 

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

–°–ª–æ–≤–æ `"hug"` –±—ã–ª–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ 10 —Ä–∞–∑ –≤ –Ω–∞—à–µ–º –∫–æ—Ä–ø—É—Å–µ —Å–ª–æ–≤, `"pug"` 5 —Ä–∞–∑,  `"pun"` 12 —Ä–∞–∑, `"bun"` 4 —Ä–∞–∑–∞, `"hugs"` 5 —Ä–∞–∑. 

meaning `"hug"` was present 10 times in the corpus, `"pug"` 5 times, `"pun"` 12 times, `"bun"` 4 times, and `"hugs"` 5 times. –ú—ã –Ω–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ, —Ä–∞–∑–¥–µ–ª—è—è –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ –Ω–∞ —Å–∏–º–≤–æ–ª—ã (—Ç–µ, –∫–æ—Ç–æ—Ä—ã–µ —Ñ–æ—Ä–º–∏—Ä—É—é—Ç –Ω–∞—à –Ω–∞—á–∞–ª—å–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å), —á—Ç–æ–±—ã –º—ã –º–æ–≥–ª–∏ –≤–∏–¥–µ—Ç—å –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ –∫–∞–∫ —Å–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤:

```
("h" "u" "g", 10), ("p" "u" "g", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "u" "g" "s", 5)
```

–¢–µ–ø–µ—Ä—å –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –ø–∞—Ä—ã. –ü–∞—Ä–∞ `("h", "u")` –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –≤ —Å–ª–æ–≤–∞—Ö `"hug"` –∏ `"hugs"`, —Ç–æ –µ—Å—Ç—å –≤—Å–µ–≥–æ 15 —Ä–∞–∑ –≤ –∫–æ—Ä–ø—É—Å–µ. –≠—Ç–æ –Ω–µ —Å–∞–º–∞—è —á–∞—Å—Ç–∞—è –ø–∞—Ä–∞, —Å–∞–º–∞—è —á–∞—Å—Ç–∞—è: `("u", "g")`, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –≤ —Å–ª–æ–≤–∞—Ö `"hug"`, `"pug"`, –∏ `"hugs"`, –≤—Å–µ–≥–æ 20 —Ä–∞–∑ –≤ —Å–ª–æ–≤–∞—Ä–µ.

–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º –ø–µ—Ä–≤–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –±—É–¥–µ—Ç —Ç–∞–∫–∏–º: `("u", "g") -> "ug"`, —á—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ —Å–æ—á–µ—Ç–∞–Ω–∏–µ `"ug"` –±—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω–æ –≤ –Ω–∞—à —Å–ª–æ–≤–∞—Ä—å –∏ —ç—Ç–∞ –ø–∞—Ä–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∞ –≤–æ –≤—Å–µ—Ö —Å–ª–æ–≤–∞—Ö –≤ –∫–æ—Ä–ø—É—Å–µ. –ù–∞ –¥–∞–Ω–Ω–æ–º —ç—Ç–∞–ø–µ –Ω–∞—à –∫–æ—Ä–ø—É—Å –≤—ã–≥–ª—è–¥–∏—Ç —Ç–∞–∫: 

```
Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug"]
Corpus: ("h" "ug", 10), ("p" "ug", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "ug" "s", 5)
```

–°–µ–π—á–∞—Å –º—ã –∏–º–µ–µ–º –ø–∞—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–¥–µ—Ä–∂–∞—Ç —Ç–æ–∫–µ–Ω—ã –¥–ª–∏–Ω–Ω–µ–µ –æ–¥–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞: —Ç–æ–∫–µ–Ω—ã `("h", "ug")`, –Ω–∞–ø—Ä–∏–º–µ—Ä (–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã 15 —Ä–∞–∑ –≤ –∫–æ—Ä–ø—É—Å–µ). –°–∞–º—ã–µ —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞–µ–º—ã–µ –ø–∞—Ä—ã –Ω–∞ –¥–∞–Ω–Ω–æ–º —à–∞–≥–µ —ç—Ç–æ  `("u", "n")`, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –≤ –∫–æ—Ä–ø—É—Å–µ 16 —Ä–∞–∑, —Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, —Å–ª–µ–¥—É—é—â–µ–µ –≤—ã—É—á–µ–Ω–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –±—É–¥–µ—Ç —Ç–∞–∫–∏–º: `("u", "n") -> "un"`. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –ø—Ä–∏–≤–µ–¥–µ—Ç –∫ –∑–∞–º–µ–Ω–µ (–æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—é) –≤—Å–µ—Ö –≤—Å—Ç—Ä–µ—á–µ–Ω–Ω—ã—Ö –ø–∞—Ä —Å–∏–≤–æ–ª–æ–≤ `("u", "n")`:

```
Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug", "un"]
Corpus: ("h" "ug", 10), ("p" "ug", 5), ("p" "un", 12), ("b" "un", 4), ("h" "ug" "s", 5)
```

–¢–µ–ø–µ—Ä—å —Å–∞–º–∞—è —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞–µ–º–∞—è –ø–∞—Ä–∞ —ç—Ç–æ `("h", "ug")`, —Ç–∞–∫ —á—Ç–æ –º—ã –º–æ–∂–µ–º –≤—ã—É—á–∏—Ç—å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ `("h", "ug") -> "hug"`, –∫–æ—Ç–æ—Ä–æ–µ –¥–∞—Å—Ç –Ω–∞–º –ø–µ—Ä–≤—ã–π —Ç—Ä–µ—Ö—Å–∏–º–≤–æ–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω. –ü–æ—Å–ª–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –∫–æ—Ä–ø—É—Å –±—É–¥–µ—Ç –≤—ã–≥–ª—è–¥–µ—Ç—å —Ç–∞–∫: 

```
Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug", "un", "hug"]
Corpus: ("hug", 10), ("p" "ug", 5), ("p" "un", 12), ("b" "un", 4), ("hug" "s", 5)
```

–¢–∞–∫ –±—É–¥–µ—Ç –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å—Å—è –¥–æ —Ç–µ—Ö –ø–æ—Ä, –ø–æ–∫–∞ –º—ã –Ω–µ –¥–æ—Å—Ç–∏–≥–Ω–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ —Å–ª–æ–≤–∞—Ä—è. 

<Tip>

‚úè ** –í–∞—à–∞ –æ—á–µ—Ä–µ–¥—å! ** –ö–∞–∫ –≤—ã –¥—É–º–∞–µ—Ç–µ, —á—Ç–æ –±—É–¥–µ—Ç —Å–ª–µ–¥—É—é—â–∏–º –ø—Ä–∞–≤–∏–ª–æ–º —Å–ª–∏—è–Ω–∏—è?

</Tip>

## Tokenization algorithm[[tokenization-algorithm]]

–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ —Å–ª–µ–¥—É–µ—Ç –≤—ã—É—á–µ–Ω–Ω—ã–º –ø—Ä–∞–≤–∏–ª–∞–º, –Ω–æ–≤—ã–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É—é—Ç—Å—è –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å–æ —Å–ª–µ–¥—É—â–∏–º–∏ —à–∞–≥–∞–º–∏:

1. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
2. –ü—Ä–µ-—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
3. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Å–ª–æ–≤ –Ω–∞ —Å–∏–º–≤–æ–ª—ã
4. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è (—Å–ª–∏—è–Ω–∏—è)

–î–∞–≤–∞–π—Ç–µ —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –Ω–∞—à –ø—Ä–∏–º–µ—Ä: 

```
("u", "g") -> "ug"
("u", "n") -> "un"
("h", "ug") -> "hug"
```

–°–ª–æ–≤–æ `"bug"` –±—É–¥–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –≤  `["b", "ug"]`. `"mug"` –±—É–¥–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –∫–∞–∫ `["[UNK]", "ug"]`, —Ç.–∫. —Å–∏–º–≤–æ–ª–∞ `"m"` –Ω–µ—Ç –≤ –±–∞–∑–æ–≤–æ–º —Å–ª–æ–≤–∞—Ä–µ. –¢–∞–∫–∏–º –∂–µ –æ–±—Ä–∞–∑–æ–º —Å–ª–æ–≤–æ `"thug"` –±—É–¥–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –≤ `["[UNK]", "hug"]`: —Å–∏–º–≤–æ–ª–∞ `"t"` –Ω–µ—Ç –≤ –Ω–∞—à–µ–º –±–∞–∑–æ–≤–æ–º —Å–ª–æ–≤–∞—Ä–µ, –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª —Å–ª–∏—è–Ω–∏—è –ø–æ–≤–ª–∏—è–µ—Ç –≤ –ø–µ—Ä–≤—É—é –æ—á–µ—Ä–µ–¥—å –Ω–∞ `"u"` –∏ `"g"`, –∞ –ø–æ—Å–ª–µ –∏ –Ω–∞ –ø–∞—Ä—É `"hu"` —Å `"g"`. 

<Tip>

‚úè ** –¢–µ–ø–µ—Ä—å –≤–∞—à–∞ –æ—á–µ—Ä–µ–¥—å! ** –ö–∞–∫ –≤—ã –¥—É–º–∞–µ—Ç–µ, –∫–∞–∫ —Å–ª–æ–≤–æ `"undug"` –±—É–¥–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–æ?

</Tip>

## –ò–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü–∏—è BPE[[implementing-bpe]]

–¢–µ–ø–µ—Ä—å –¥–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é –∞–ª–≥–æ—Ä–∏—Ç–º–∞ BPE. –≠—Ç–æ –Ω–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è, –∫–æ—Ç–æ—Ä—É—é –≤—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤ –±–æ–ª—å—à–æ–º –∫–æ—Ä–ø—É—Å–µ; –Ω–∞—à–∞ –∑–∞–¥–∞—á–∞ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–æ–¥ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–∞. 

–°–Ω–∞—á–∞–ª–∞ –Ω–∞–º –Ω—É–∂–µ–Ω –∫–æ—Ä–ø—É—Å, —Ç–∞–∫ —á—Ç–æ –¥–∞–≤–∞–π—Ç–µ —Å–æ–∑–¥–∞–¥–∏–º –ø—Ä–æ—Å—Ç–æ–π –∫–æ—Ä–ø—É—Å —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏:

```python
corpus = [
    "This is the Hugging Face Course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

–î–∞–ª–µ–µ –Ω–∞–º –Ω—É–∂–Ω–æ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –∑–∞–∫—Ä–µ–ø–∏—Ç—å —ç—Ç–æ—Ç –∫–æ—Ä–ø—É—Å –≤ —Å–ª–æ–≤–∞—Ö. –ü–æ—Å–∫–æ–ª—å–∫—É –º—ã —Ä–µ–ø–ª–∏—Ü–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä BPE (–Ω–∞–ø—Ä–∏–º–µ—Ä, GPT-2), –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä `gpt2` –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("gpt2")
```

–ó–∞—Ç–µ–º –º—ã –≤—ã—á–∏—Å–ª—è–µ–º —á–∞—Å—Ç–æ—Ç—ã –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ –≤ –ø–æ–ª—É—á–µ–Ω–Ω–æ–º –∫–æ—Ä–ø—É—Å–µ:

```python
from collections import defaultdict

word_freqs = defaultdict(int)

for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

print(word_freqs)
```

```python out
defaultdict(int, {'This': 3, 'ƒ†is': 2, 'ƒ†the': 1, 'ƒ†Hugging': 1, 'ƒ†Face': 1, 'ƒ†Course': 1, '.': 4, 'ƒ†chapter': 1,
    'ƒ†about': 1, 'ƒ†tokenization': 1, 'ƒ†section': 1, 'ƒ†shows': 1, 'ƒ†several': 1, 'ƒ†tokenizer': 1, 'ƒ†algorithms': 1,
    'Hopefully': 1, ',': 1, 'ƒ†you': 1, 'ƒ†will': 1, 'ƒ†be': 1, 'ƒ†able': 1, 'ƒ†to': 1, 'ƒ†understand': 1, 'ƒ†how': 1,
    'ƒ†they': 1, 'ƒ†are': 1, 'ƒ†trained': 1, 'ƒ†and': 1, 'ƒ†generate': 1, 'ƒ†tokens': 1})
```

–°–ª–µ–¥—É—é—â–∏–π —à–∞–≥: —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –±–∞–∑–æ–≤–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è, —Ç.–µ. –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –≤ –Ω–µ–º –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç: 

```python
alphabet = []

for word in word_freqs.keys():
    for letter in word:
        if letter not in alphabet:
            alphabet.append(letter)
alphabet.sort()

print(alphabet)
```

```python out
[ ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's',
  't', 'u', 'v', 'w', 'y', 'z', 'ƒ†']
```

–ú—ã —Ç–∞–∫–∂–µ –¥–æ–±–∞–≤–∏–ª–ª–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –º–æ–¥–µ–ª—å—é, –≤ –Ω–∞—á–∞–ª–æ –Ω–∞—à–µ–≥–æ —Å–ª–æ–≤–∞—Ä—è. –í —Å–ª—É—á–∞–µ GPT-2 —ç—Ç–æ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω –æ–¥–∏–Ω: `"<|endoftext|>"`.  

```python
vocab = ["<|endoftext|>"] + alphabet.copy()
```

–¢–µ–ø–µ—Ä—å –Ω–∞–º –Ω—É–∂–Ω–æ —Ä–∞–∑–¥–µ–ª–∏—Ç—å –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã, —á—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ:

```python
splits = {word: [c for c in word] for word in word_freqs.keys()}
```

–¢–µ–ø–µ—Ä—å, –∫–æ–≥–¥–∞ –º—ã –≥–æ—Ç–æ–≤—ã –∫ –æ–±—É—á–µ–Ω–∏—é, –¥–∞–≤–∞–π—Ç–µ –Ω–∞–ø–∏—à–µ–º —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –≤—ã—á–∏—Å–ª—è–µ—Ç —á–∞—Å—Ç–æ—Ç—É –∫–∞–∂–¥–æ–π –ø–∞—Ä—ã. –ù–∞–º –Ω—É–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç–æ –Ω–∞ –∫–∞–∂–¥–æ–º —ç—Ç–∞–ø–µ –æ–±—É—á–µ–Ω–∏—è:

```python
def compute_pair_freqs(splits):
    pair_freqs = defaultdict(int)
    for word, freq in word_freqs.items():
        split = splits[word]
        if len(split) == 1:
            continue
        for i in range(len(split) - 1):
            pair = (split[i], split[i + 1])
            pair_freqs[pair] += freq
    return pair_freqs
```

–î–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ —á–∞—Å—Ç—å —ç—Ç–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è –ø–æ—Å–ª–µ –ø–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω—ã—Ö —Ä–∞–∑–±–∏–µ–Ω–∏–π:

```python
pair_freqs = compute_pair_freqs(splits)

for i, key in enumerate(pair_freqs.keys()):
    print(f"{key}: {pair_freqs[key]}")
    if i >= 5:
        break
```

```python out
('T', 'h'): 3
('h', 'i'): 3
('i', 's'): 5
('ƒ†', 'i'): 2
('ƒ†', 't'): 7
('t', 'h'): 3
```

–î–ª—è –ø–æ–∏—Å–∫–∞ —Å–∞–º–æ–π —á–∞—Å—Ç–æ–π –ø–∞—Ä—ã —Å–¥–µ–ª–∞–º —Ü–∏–∫–ª: 

```python
best_pair = ""
max_freq = None

for pair, freq in pair_freqs.items():
    if max_freq is None or max_freq < freq:
        best_pair = pair
        max_freq = freq

print(best_pair, max_freq)
```

```python out
('ƒ†', 't') 7
```

–ü–µ—Ä–≤–æ–µ –≤—ã—É—á–µ–Ω–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ: `('ƒ†', 't') -> 'ƒ†t'`, –¥–æ–±–∞–≤–ª—è–µ–º `'ƒ†t'` –≤ —Å–ª–æ–≤—Ä—å:

```python
merges = {("ƒ†", "t"): "ƒ†t"}
vocab.append("ƒ†t")
```

–î–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø—Ä–∏–º–µ–Ω–∏—Ç—å —ç—Ç–æ —Å–ª–∏—è–Ω–∏–µ –≤ –Ω–∞—à–µ–º —Å–ª–æ–≤–∞—Ä–µ. –ù–∞–ø–∏—à–µ–º –µ—â–µ –æ–¥–Ω—É —Ñ—É–Ω–∫—Ü–∏—é: 

```python
def merge_pair(a, b, splits):
    for word in word_freqs:
        split = splits[word]
        if len(split) == 1:
            continue

        i = 0
        while i < len(split) - 1:
            if split[i] == a and split[i + 1] == b:
                split = split[:i] + [a + b] + split[i + 2 :]
            else:
                i += 1
        splits[word] = split
    return splits
```

–¢–µ–ø–µ—Ä—å –º—ã –º–æ–∂–µ–º –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–µ—Ä–≤–æ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è: 


```py
splits = merge_pair("ƒ†", "t", splits)
print(splits["ƒ†trained"])
```

```python out
['ƒ†t', 'r', 'a', 'i', 'n', 'e', 'd']
```

–¢–µ–ø–µ—Ä—å —É –Ω–∞—Å –µ—Å—Ç—å –≤—Å–µ, —á—Ç–æ –Ω–∞–º –Ω—É–∂–Ω–æ, —á—Ç–æ–±—ã —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ü–∏–∫–ª, –≤ –∫–æ—Ç–æ—Ä–æ–º –º—ã –Ω–∞–π–¥–µ–º –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Å–ª–∏—è–Ω–∏—è. –ë—É–¥–µ–º —Å—Ç—Ä–µ–º–∏—Ç—å—Å—è –∫ —Ä–∞–∑–º–µ—Ä—É —Å–ª–æ–≤–∞—Ä—è –≤ 50 —ç–ª–µ–º–µ–Ω—Ç–æ–≤. 

```python
vocab_size = 50

while len(vocab) < vocab_size:
    pair_freqs = compute_pair_freqs(splits)
    best_pair = ""
    max_freq = None
    for pair, freq in pair_freqs.items():
        if max_freq is None or max_freq < freq:
            best_pair = pair
            max_freq = freq
    splits = merge_pair(*best_pair, splits)
    merges[best_pair] = best_pair[0] + best_pair[1]
    vocab.append(best_pair[0] + best_pair[1])
```

–í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –º—ã –≤—ã—É—á–∏–ª–∏ 19 –ø—Ä–∞–≤–∏–ª –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è (–Ω–∞—á–∞–ª—å–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å –∏–º–µ–ª —Ä–∞–∑–º–µ—Ä 31: 30 —Å–∏–º–≤–æ–ª–æ–≤ –∞–ª—Ñ–∞–≤–∏—Ç–∞ –∏ –æ–¥–∏–Ω —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω):

```py
print(merges)
```

```python out
{('ƒ†', 't'): 'ƒ†t', ('i', 's'): 'is', ('e', 'r'): 'er', ('ƒ†', 'a'): 'ƒ†a', ('ƒ†t', 'o'): 'ƒ†to', ('e', 'n'): 'en',
 ('T', 'h'): 'Th', ('Th', 'is'): 'This', ('o', 'u'): 'ou', ('s', 'e'): 'se', ('ƒ†to', 'k'): 'ƒ†tok',
 ('ƒ†tok', 'en'): 'ƒ†token', ('n', 'd'): 'nd', ('ƒ†', 'is'): 'ƒ†is', ('ƒ†t', 'h'): 'ƒ†th', ('ƒ†th', 'e'): 'ƒ†the',
 ('i', 'n'): 'in', ('ƒ†a', 'b'): 'ƒ†ab', ('ƒ†token', 'i'): 'ƒ†tokeni'}
```

–¢–µ–ø–µ—Ä—å —Å–ª–æ–≤–∞—Ä—å —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞, –∏–∑–Ω–∞—á–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∞–ª—Ñ–∞–≤–∏—Ç–∞ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è: 

```py
print(vocab)
```

```python out
['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o',
 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'ƒ†', 'ƒ†t', 'is', 'er', 'ƒ†a', 'ƒ†to', 'en', 'Th', 'This', 'ou', 'se',
 'ƒ†tok', 'ƒ†token', 'nd', 'ƒ†is', 'ƒ†th', 'ƒ†the', 'in', 'ƒ†ab', 'ƒ†tokeni']
```

<Tip>

üí° –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ `train_new_from_iterator ()` –ù–∞ —Ç–æ–º –∂–µ –∫–æ—Ä–ø—É—Å–µ –Ω–µ –ø—Ä–∏–≤–µ–¥–µ—Ç –∫ —Ç–∞–∫–æ–º—É –∂–µ —Å–ª–æ–≤–∞—Ä—é. –≠—Ç–æ —Å–≤—è–∑–∞–Ω–æ —Å —Ç–µ–º, —á—Ç–æ, –∫–æ–≥–¥–∞ –º—ã –≤—ã–±–∏—Ä–∞–ª–∏ —Å–∞–º—É—é —á–∞—Å—Ç—É—é –ø–∞—Ä—É, –º—ã –≤—ã–±—Ä–∞–ª–∏ –ø–µ—Ä–≤—É—é –≤—Å—Ç—Ä–µ—á–Ω—É—é, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ ü§ó Tokenizers –≤—ã–±–∏—Ä–∞–µ—Ç –ø–µ—Ä–≤—É—é –ø–∞—Ä—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –µ—ë –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞.

</Tip>

–ß—Ç–æ–±—ã —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å –Ω–æ–≤—ã–π —Ç–µ–∫—Å—Ç, –º—ã –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –µ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º, —Ä–∞–∑–¥–µ–ª—è–µ–º –µ–≥–æ, –ø–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω—è–µ–º –≤—Å–µ –∏–∑—É—á–µ–Ω–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ —Å–ª–∏—è–Ω–∏—è:

```python
def tokenize(text):
    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in pre_tokenize_result]
    splits = [[l for l in word] for word in pre_tokenized_text]
    for pair, merge in merges.items():
        for idx, split in enumerate(splits):
            i = 0
            while i < len(split) - 1:
                if split[i] == pair[0] and split[i + 1] == pair[1]:
                    split = split[:i] + [merge] + split[i + 2 :]
                else:
                    i += 1
            splits[idx] = split

    return sum(splits, [])
```

–ú—ã –º–æ–∂–µ–º –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —ç—Ç–æ –Ω–∞ –ª—é–±–æ–º —Ç–µ–∫—Å—Ç–µ, —Å–æ—Å—Ç–æ—è—â–µ–º –∏–∑ —Å–∏–º–≤–æ–ª–æ–≤ –∞–ª—Ñ–∞–≤–∏—Ç–∞:

```py
tokenize("This is not a token.")
```

```python out
['This', 'ƒ†is', 'ƒ†', 'n', 'o', 't', 'ƒ†a', 'ƒ†token', '.']
```

<Tip warning={true}>

‚ö† –ù–∞—à–∞ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–≤–µ–¥–µ—Ç –∫ –æ—à–∏–±–∫–µ, –µ—Å–ª–∏ –µ—Å—Ç—å –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Å–∏–º–≤–æ–ª, —Ç–∞–∫ –∫–∞–∫ –º—ã –Ω–∏—á–µ–≥–æ –Ω–µ —Å–¥–µ–ª–∞–ª–∏, —á—Ç–æ–±—ã —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è —Å —ç—Ç–æ–π —Å–∏—Ç—É–∞—Ü–∏–µ–π. GPT-2 –Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ –Ω–µ –∏–º–µ–µ—Ç —Ç–æ–∫–µ–Ω–∞ –¥–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞ (–Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Å–∏–º–≤–æ–ª –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ BPE –Ω–∞ —É—Ä–æ–≤–Ω–µ –±–∞–π—Ç–æ–≤), –Ω–æ —ç—Ç–æ –º–æ–∂–µ—Ç –ø—Ä–æ–∏–∑–æ–π—Ç–∏ –∑–¥–µ—Å—å, –ø–æ—Ç–æ–º—É —á—Ç–æ –º—ã –Ω–µ –≤–∫–ª—é—á–∏–ª–∏ –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –±–∞–π—Ç—ã –≤ –Ω–∞—á–∞–ª—å–Ω—ã–π —Å–ª–æ–≤–∞—Ä–Ω—ã–π –∑–∞–ø–∞—Å. –≠—Ç–æ—Ç –∞—Å–ø–µ–∫—Ç BPE –≤—ã—Ö–æ–¥–∏—Ç –∑–∞ —Ä–∞–º–∫–∏ —ç—Ç–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∞, –ø–æ—ç—Ç–æ–º—É –º—ã –æ–ø—É—Å—Ç–∏–ª–∏ –¥–µ—Ç–∞–ª–∏.

</Tip>

–¢–∞–∫ —É—Å—Ç—Ä–æ–µ–Ω –∞–ª–≥–æ—Ä–∏—Ç–º BPE! –î–∞–ª–µ–µ –º—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –∞–ª–≥–æ—Ä–∏—Ç–º WordPiece. 
